%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Template for using the ltx-talk package to create accessible
%   course slides from former beamer content
%
%   First 2 "chapters" of Spring 2026 ST 790, Statistical Methods for
%   Analysis With Missing Data
%
%   Demonstrates how to structure your content to use the ltx-talk
%   package to create slides that pass PDF accessibility checkers that
%   can handle mathml content AND that DELTA has reviewed and agrees
%   meets their accessibility standards.
%
%   IMPORTANT:  To obtain an accessible pdf, you must include all of
%   the stuff I've indicated below.  Also, you MUST compile your LaTEX
%   source using lualatex (which is included in the Texlive 2025
%   distribution and Overleaf; see below).  DO NOT use pdflatex or
%   some other engine.
%
%   BE PATIENT:  When you process a ltx-talk document (or any other
%   LaTEX document for that matter) using lualatex, it will take some
%   time to process and build the tagging structure.  See also the
%   notes below.
%
%   ltx-talk uses the SAME syntax to create slides as beamer:
%
%   \begin{frame}
%     \frametitle{title}
%
%   \end{frame}
%
%   Thus, converting your beamer slides to work in ltx-talk is pretty
%   straightforward.  You will probably have to prettify the results
%   manually, as things might not turn out exactly to be formatted
%   exactly the same as in beamer.
%
%  OVERLAYS:  I don't use overlays in these notes, but if you use them
%  in beamer, SEE THE DOCUMENTATION (link belw) -- ltx-talk DOES
%  SUPPORT OVERLAYS!
%
%  The very last slide below has an example of creating a very simple
%  table.  The pdf accessibility checkers are finicky about tables.
%  The safest approach, which is used below, is to create your tables
%  so that they have one header row at the top and optionally a header
%  column on the left.  Do not have headers that merge cells, as that
%  seems to create havoc.
%
%  See

%https://esail.tamu.edu/faculty-tutorials/accessible-latex-pdf-ua-2-overleaf-2025/

%  for an example of using \tabstruct and tabularx to create a fancier
%  table.
%
%
%   Last modified 01/12/2026
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  Document meta data  -- needed for tagging, etc - must be located
%%  BEFORE \documentclass -- see this page at Texas A&M
%%  https://esail.tamu.edu/faculty-tutorials/accessible-latex-pdf-ua-2-overleaf-2025/

%%  As noted in the Texas A&M instructions, DO NOT USE ADOBE ACROBAT,
%%  PAC, FOXIT, ETC TO CHECK ACCESSIBILTY -- THEY DO NOT HANDLE
%%  MATHML.  THEY RECOMMEND USING VERA PDF (the DELTA folks are using
%%  it to evaluate pdfs with math content).  THE VERA PDF ONLINE
%%  ACCESSIBILTY CHECKER IS AT https://dev.verapdf-rest.duallab.com/

%%  It's self explanatory.  Choose HTML Output Format (and be patient)
%%  -- it will produce a report you can download.  What you want to
%%  see is GREEN -- everything Passed.

%%  ltx-talk works with the xcolor package
\usepackage{xcolor}

%% Need to be sure that if you use colors, the contrast with a
%%  white background is acceptable.  In the xcolor package, the basic
%%  blue is acceptable, but red is not.  However, we define "ncsured"
%%  here, which is an official NCSU brand color that is acceptable!  I
%%  use it for slide headings and other emphasis
\definecolor{ncsured}{RGB}{153,0,0}  % NCSU brand color deep reddish maroon

\DocumentMetadata{
  tagging = on,
lang = en,
pdfstandard = ua-2,
  pdfstandard = a-4f, % optional archival standard
  tagging-setup = {math/setup={mathml-AF,mathml-SE},
                   extra-modules={verbatim-mo},
                   table/header-rows=1}}

% The ltx-talk package is available in the latest version of the 2025
% Texlive distribution, which is included in Overleaf or can be
% downloaded at this website:        https://tug.org/texlive/

% The documentation is available here:
% https://ctan.org/pkg/ltx-talk?lang=en

%  If you are on a Mac and already have texlive 2025 installed, you
%  will need to update your installation, which you can do as
%  superuser by running from a command prompt:

%  sudo tlmgr update --all

%  It will take quite some time to download everything.  You may need
%  to update the update manager first if it doesn't work: sudo tlmgr
%  update --self                 
               
\documentclass{ltx-talk}

%% This is an ltx-talk command that formats the footer of a slide --
%% see the examples link in the ltx-talk documentation for
%% instructions on how to specify.  You can use colors.

\EditInstance{footer}{std}{
  color         = black!50,
  separator     = \hfill|\hfill,
  element-order = { title, framenumber},
  right-hspace  = 3mm,
  left-hspace   = 3mm,
}

%% Format the header of a slide -mine is pretty simple.  See the
%% documentation if you want to be fancier.
\EditInstance{header}{std}{
  color            = ncsured,
}

\usepackage{amsmath,amsfonts}
\usepackage{amssymb}
\usepackage{bbding}

%% To get accessible math, you MUST include the unicode-math package
%% AFTER amsmath.  When you process your document using lualatex,
%% some errors will occur early because there are some commands that
%% are already defined in the ams packages that are redefined in
%% unicode-math.  JUST IGNORE THIS AND LET IT KEEP PROCESSING

\usepackage{unicode-math}

%%  You need hyperref to create the cross references
\usepackage{hyperref}

%%  As always, you need graphicx to \includegraphics -- see the
%%  examples of including images later in this template for how to
%%  specify an alt= description to tag your figures/images
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%  NOTATION  %%%%%%%%%%%%%%%%%%%%%  

%%  These are newcommands and defs I use for my missing data course;
%%  put yours here

\newcommand{\bbt}{$\blacktriangleright$}

\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\emf}{\sf \em}
\newcommand{\embf}{\sf \textbf}

\newcommand{\bfem}[1]{{\em #1}}  % for slides

\newcommand{\simdot}{\stackrel{\cdot}{\sim}}

\newcommand{\var}{\mbox{var}}
\newcommand{\SE}{\mbox{SE}}
\newcommand{\cov}{\mbox{cov}}
\newcommand{\tr}{\mbox{tr}}
\newcommand{\corr}{\mbox{corr}}
\newcommand{\pr}{\mbox{pr}}

\newcommand{\dss}{\displaystyle}

\newcommand{\N}{\mathcal{N}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\sumiN}{\sum^N_{i=1}}
\newcommand{\sumjT}{\sum^T_{j=1}}
\newcommand{\summM}{\sum^M_{m=1}}

\newcommand{\Zobs}{Z^{obs}}
\newcommand{\Zmis}{Z^{mis}}
\newcommand{\zobs}{z^{obs}}
\newcommand{\zmis}{z^{mis}}

\newcommand{\Yobs}{Y^{obs}}
\newcommand{\Ymis}{Y^{mis}}
\newcommand{\yobs}{y^{obs}}
\newcommand{\ymis}{y^{mis}}

%  Data from N individuals

\newcommand{\utilde}{\underline}
\newcommand{\onetil}{\utilde{1}}

\newcommand{\Ytil}{\utilde{Y}}
\newcommand{\Ztil}{\utilde{Z}}
\newcommand{\ztil}{\utilde{z}}
\newcommand{\Rtil}{\utilde{R}}
\newcommand{\ZRtil}{\Ztil_{(\Rtil)}}
\newcommand{\rtil}{\utilde{r}}
\newcommand{\Zrtil}{\Ztil_{(\rtil)}}
\newcommand{\zrtil}{\ztil_{(\rtil)}}

\newcommand{\hatY}{\hat{Y}}

\newcommand{\hatalpha}{\hat{\alpha}}
\newcommand{\hatsigma}{\hat{\sigma}}
\newcommand{\hatbeta}{\hat{\beta}}
\newcommand{\hatmu}{\hat{\mu}}
\newcommand{\hattheta}{\hat{\theta}}
\newcommand{\hateta}{\hat{\eta}}
\newcommand{\hatpsi}{\hat{\psi}}
\newcommand{\hatxi}{\hat{\xi}}
\newcommand{\hatgamma}{\hat{\gamma}}
\newcommand{\hatSigma}{\hat{\Sigma}}

\newcommand{\hatthetam}{\hat{\theta}^{(m)}}
\newcommand{\hatthetamstar}{\hat{\theta}^{*(m)}}
\newcommand{\Zim}{Z^{(m)}_i}
\newcommand{\hatthetainit}{\hat{\theta}^{(init)}}

\newcommand{\Zr}{Z_{(r)}}
\newcommand{\Zri}{Z_{(r)i}}
\newcommand{\ZR}{Z_{(R)}}
\newcommand{\ZRi}{Z_{(R_i)i}}
\newcommand{\Rbar}{\overline{R}}
\newcommand{\rbar}{\bar{r}}
\newcommand{\Zrbar}{Z_{(\rbar)}}
\newcommand{\Zrbari}{Z_{(\rbar)i}}
\newcommand{\ZRbar}{Z_{(\Rbar)}}
\newcommand{\Zj}{Z_{(j)}}
\newcommand{\Zji}{Z_{(j)i}}
\newcommand{\ZD}{Z_{(D)}}
\newcommand{\ZDi}{Z_{(D_i)i}}
\newcommand{\zr}{z_{(r)}}
\newcommand{\zrbar}{z_{(\rbar)}}
\newcommand{\zrbari}{z_{(\rbar)i}}
\newcommand{\jbar}{\bar{j}}
\newcommand{\Zjbar}{Z_{(\jbar)}}
\newcommand{\zj}{z_{(j)}}
\newcommand{\zjbar}{z_{(\jbar)}}
\newcommand{\Zell}{Z_{(\ell)}}
\newcommand{\zell}{z_{(\ell)}}


\newcommand{\tilY}{\tilde{Y}}
\newcommand{\Yj}{Y_{(j)}}
\newcommand{\Yjbar}{Y_{(\bar{j})}}

\newcommand{\mur}{\mu_{(r)}}
\newcommand{\murbar}{\mu_{(\rbar)}}
\newcommand{\Yr}{Y_{(r)}}
\newcommand{\Yri}{Y_{(r)i}}
\newcommand{\Yrbar}{Y_{(\rbar)}}
\newcommand{\yrbar}{y_{(\rbar)}}
\newcommand{\Yrbari}{Y_{(\rbar)i}}
\newcommand{\Yrii}{Y_{(r_i)}}
\newcommand{\Yrbarii}{Y_{(\rbar_i)i}}
\newcommand{\St}{S_\theta}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
%\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\inp}{\stackrel{p}{\longrightarrow}}
\newcommand{\inL}{\stackrel{\mathcal{L}}{\longrightarrow}}

\newcommand{\ds}{\displaystyle}

%%%%%%%%%%%%%%%   Color emphasis commands  %%%%%%%%%%%%%

%% If you want to have color, you can use xcolor and \textcolor to do
%% it.  I defined these newcommands to emphasize headings and passages
%% in the text with boldface ncsured and italic blue (see the slides for
%% how this looks)

\newcommand{\redbf}[1]{\textbf{\textcolor{ncsured}{#1}}}
\newcommand{\blueem}[1]{\textit{\textcolor{blue}{#1}}}

%%%%%%%%%%%%%%%%  Heading structure  %%%%%%%%%%%%%%%%%%%%%%

%%  In my back and forth with DELTA,  we found that the usual way one
%%  would create these slides, with a title slide created
%%  with \maketitle and then organizing the contact by \sections and
%%  then \subsections to create bookmarks and a TOC messed up the
%%  heading structure, where there needs to be a hiearchy with H1 at
%%  the top, H2 next, H3 next, etc.  Thus, in what appears below, I do
%%  not use the \maketitle command and thus the way of making the
%%  title slide as in the ltx-talk documentation (similar to how you
%%  would do it in beamer).  Instead, I make it manually.  Somehow
%%  the \maketitle is screwing up the heading structure.

%%  The workaround we came up with involves having a single \section
%%  (which I made the title of the course), which creates the H1
%%  heading, with "sections" of the course (I call these "Chapters" in
%%  the content) then created as \subsections (H2 heading), followed
%%  by "subsections" created by \subsubsections (H3 heading).  The H4
%%  heading is the slides themselves.  Thus:

%% This document has one section 1 and 7 subsections to satisfy the
%% hierarchy for headers (H1 is section, H2 is subsection, H3 is
%% subsubsection, H4 is frame).  The subsections are my "chapters."
%% LaTEX numbers subsections within section 1 as 1.1., 1.2, etc, so
%% that the equation numbers are (1.1.x) in the first subsection,
%% (1.2.x) in the second, etc.  But I want equation numbers (1.x) in
%% the first, (2.x) in the second, etc, without the section number
%% appended in front.  The following using the chngcntr package and
%% numberwithin from amsmath does this correctly.

\usepackage{chngcntr}
\counterwithout{subsection}{section}
\renewcommand{\theequation}{\thesubsection.\arabic{equation}}

\numberwithin{equation}{subsection}

%% This will set the document properties.  Give your document a
%% meaningful title (needed to pass accessibility).

\hypersetup{ 
  pdftitle={Statistical Methods for Analysis With Missing Data}, 
  pdfauthor={davidian@ncsu.edu}, 
  pdfsubject={Course notes}, 
  pdfkeywords={Missing at random, not missing at random, nonignorable}, 
  pdfdisplaydoctitle=true 
}

%% As above, the title slide messes up the hierarchy of headers in the
%% tagged pdf according to DELTA.  So make your own title slide and
%% have an overarching "section," which will correctly be labeled H1.
%% Then sub and subsub sections get labeled H2 and H3, and frames are
%% H4

%% The next commented out stuff shows how you would make a title slide
%% if not for the heading weirdness

%%%%%%%%%%%%%%%%%%  TITLE SLIDE  %%%%%%%%%%%%%%%%%

% \title{\textbf{Statistical Methods for Analysis With Missing Data}}
% \author{ }

% \date{   }

% \institute{Department of Statistics \\ North Carolina State University}

 \begin{document}

% %%
% %% Titlepage gets created
% %%
% \begin{frame}
% \maketitle
% \end{frame}

%%  Here is the overarching \section to create H1 -- it is the
%%  only \section in the document
 
\section*{Statistical Methods for Analysis With Missing Data}

%%%%%%%%%%%%%%   Manually created title slide:

\begin{frame}
  \frametitle{  }

  \begin{center}
    \textbf{\Large \textcolor{ncsured}{Statistical Methods for Analysis With Missing Data} }\\*[0.2in]

    Department of Statistics \\
    North Carolina State University\\

    \vspace{0.1in}
    \includegraphics[height=2cm,alt={NC State Department of Statistics
    logo}]{stat-logo.jpg}
    \end{center}

  \end{frame}

  %%%%%%%%%%%%%%%% CONTENT STARTS  HERE %%%%%%%%%%%%%

%%  For illustration, this document contains the first 2 chapters of
%%  content of ST 790 plus the following  "Preliminaries" section.  As
%%  a result, there are a few cross references to later material that
%%  will not work if you were to attempt to compile this.   Note that
%%  I create some cross references to slides using \pageref, where the
%%  label for the cross referenced slide is on that slide (search on
%%  Slide" to see the example below).  

%%  My first hunk of content "Preliminaries" is actually preliminary
%%  information.  I want the actual course content starting with
%%  "Chapter 1" to be numbered starting with 1.  So because "Chapters"
%%  are being created as "subsections," I set section counter so that
%%  subsection) numbers start with 1
  
\setcounter{subsection}{-1}

\subsection{Preliminaries}

\subsubsection{Course overview}

\begin{frame}
\frametitle{Course Objective}

\redbf{Background:}
\begin{itemize}
\item Missing data are \blueem{ubiquitous} in almost every area
of scientific inquiry, and especially in health sciences research
involving \blueem{human subjects}

\item At the very least, missing data lead to a \blueem{loss of precision
  of inference} on the population of interest relative to that intended

  \item Of much greater concern is the potential for \blueem{biased and misleading
inferences} that can result if the reasons for missingness are related
to outcomes of interest

\item Principled methods to take this challenge into appropriate
  account are required
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Course Objective}

\redbf{Goal of course:}
\begin{itemize}
\item Provide an overview of \blueem{fundamental statistical
  frameworks and methods} for analysis in the presence of missing data

 \item  Give students the \blueem{foundation} to read the current literature and
  have broad appreciation the implications of missing data for valid
  inference
\end{itemize}

\vspace*{0.1in}

\redbf{Topics:}  Missing data mechanisms; ad hoc methods (e.g.,
complete case and last value carried forward analyses); methods under
the assumption of missing at random: likelihood-based methods and the
EM algorithm, multiple imputation, and inverse probability weighting;
methods under nonignorable missingness; methods for
sensitivity analysis
\begin{itemize}
\item Use of the methods \blueem{in practice} is emphasized

\item \blueem{Implementation} in SAS and R is integrated throughout
\end{itemize}
\end{frame}

%% Create the Course Outline as a table of contents -- I include only
%% the main sections (As above I call them Chapters in the course content)

%%  Make so that table of contents has only sections 
\setcounter{tocdepth}{2}

\begin{frame}
\frametitle{Course Outline}
\begin{footnotesize}
  %\tableofcontents[sections=\value{section}]
  \tableofcontents
\end{footnotesize}
\end{frame}

%% set tocdepth counter to 3 so you get subsections in bookmarks
\setcounter{tocdepth}{3}

%%%%%%%%%%%%%%%%%%%%%%%%%   ALL CHAPTERS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%   CHAPTER 1

\subsection{Introduction and Motivation}
\label{s:intro}

\subsubsection{Fundamental problem}

%% Unfortunately ltx-talk does not have the capability as beamer does
%% to produce fancy tables of contents and repeated tables of contents
%% that show you were you are section-wise, for example, by graying
%% out all section headings except the one that comes next.  At least
%% I couldn't figure out how to do that.  So I settled for manually
%% creating a slide immediately following each subsection (created
%% by \subsubsection) for each section (what I refer to in these notes
%% as "chapters," created by \subsection) to indicate where we are in
%% the notes.

\begin{frame}
  \frametitle{1.  Introduction and Motivation}
  %\label{s:intro}

 \redbf{1.1.  Fundamental Problem}
  
  \end{frame}
  
\begin{frame}
\frametitle{Big picture}

\redbf{In practice:}  In a range of settings, such as clinical
trials, sample surveys, agricultural experiments, and more, \blueem{
  data are to be collected according to a predetermined plan or
  experimental design}.
\begin{itemize}
\item Given these data, the objective is to \blueem{make inference
    about some aspect of an underlying population of interest}
\end{itemize}

\vspace*{0.2in}

\redbf{However:} Data that are intended to be collected \blueem{may not be
  collected or may not not available}\,, for example\ldots


\end{frame}

\begin{frame}
  \frametitle{Big picture}

  \redbf{Dropout and noncompliance in a clinical trial:} A clinical
  trial to compare $\geq 2$ treatments in a certain patient population
  \begin{itemize}
  \item Subjects are randomly assigned to one of the treatments

  \item Subjects are \blueem{supposed to} to take the treatment as
    directed and return to weekly for an outcome measure to be
    recorded

  \item Some subjects \blueem{may fail to attend clinic visits} beyond a
    certain point, \blueem{dropping out} of the study

    \item Some subjects  \blueem{may quit taking their treatments as directed or quit taking them
        altogether}

   \item Some subjects \blueem{may miss clinic visits sporadically}

   \item \blueem{Result:} Part of the \blueem{intended full set of
     longitudinal outcome} arising from taking assigned treatment and
     visiting the clinic as directed will be \blueem{missing} for
     these subjects
   \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Big picture}

  \redbf{Surrogate measurements and missingness by design:} In a
  nutrition study, \blueem{daily average percent fat intake} in a certain
  population is of interest, and a random sample of subjects from
  the population is recruited to participate
  \begin{itemize}
\item \blueem{Expensive/burdensome outcome measure:}  Long-term fat
  intake taken from a detailed \blueem{food diary}  (accurate measurement)

\item \blueem{Simpler/cheaper outcome measure:} \blueem{24-hour
    recall} of food eaten (much less accurate)

\item Outcome from 24-hour recall is likely \blueem{correlated with}
  food diary outcome but is an \blueem{error-prone} version of it

  \item \blueem{Surrogate outcome}

    \item \blueem{Study design:} To reduce costs/burden, \blueem{
        all subjects} provide a 24-hour recall outcome, with a 
      \blueem{random subsample} also providing the food diary outcome
      (\blueem{validation sample}\,)

    \item \blueem{Result:} The expensive measure will be \blueem{
        missing} for all subjects not in the validation sample
    \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Big picture}

  \redbf{Missingness:} Can arise in different ways
  \begin{itemize}

  \item \blueem{By happenstance} in the clinical trial example

    \item \blueem{By design} in the nutrition study example
    \end{itemize}

\vspace*{0.1in}

\redbf{Fact of life:}  In almost all studies involving \blueem{human subjects}
\begin{itemize}
\item There will be \blueem{missing data} 

  \item So that dealing with missing data is a \blueem{routine
      challenge} for the data analyst
 \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Challenge}

\redbf{More formally:}  Interest is in inference on some aspect of the
distribution of the \blueem{full data} that were intended to be collected 
\begin{itemize}
\item A (random) sample from this distribution is available if there
  are no missing data $ \Longrightarrow$ inference is \blueem{
    straightforward}

\item But if some intended data are \blueem{missing}\, depending on
  the nature of the missingness, the \blueem{available, observed
    data} may not be a random sample $ \Longrightarrow$ inference
  may be \blueem{compromised}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Challenge}

  \redbf{Possible approaches:}
  \begin{itemize}
\item \blueem{Ignore} the problem, analyze the \blueem{observed
    data} as if they were the \blueem{intended, full data} $
  \Longrightarrow$ can lead to \blueem{misleading conclusions}

  \item \blueem{Acknowledge} the problem and try to ``\blueem{
      correct for}\,'' the missingness somehow

  \item \blueem{History:}  Missing data were originally viewed as a
    \blueem{computational challenge}\,; e.g., inducing \blueem{
      imbalance} in a designed experiment that complicates calculations

  \item \blueem{1970s:} \blueem{Formal statistical approaches}
    began to be developed

    \item \blueem{Fundamental step:}  Rubin (1976) presented a
      \blueem{formal statistical framework} for thinking about
      missing data that is the basis for modern statistical methods
   
    \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Objective, restated}

  \redbf{Review of fundamental developments in statistical models
    and methods:}
  \begin{itemize}
\item Formal framework, assumptions, terminology

\item ``Na\"{i}ve'' methods and their drawbacks

\item Main classes of methods for valid inference under clearly stated
  assumptions about how missingness data arise

  \item Methods for assessing the sensitivity of inferences to
  departures from assumptions on how missing data arise
     \end{itemize}
   \end{frame}

\subsubsection{Examples}
   
   \begin{frame}
  \frametitle{1.  Introduction and Motivation}
  %\label{s:intro}

 \redbf{1.2.  Examples}
  
  \end{frame}
  
\begin{frame}
  \frametitle{Nonresponse in sample surveys}

  \redbf{Sample survey:}  To characterize the \blueem{financial status of
  households} in the US population
  \begin{itemize}
  \item Questionnaire sent to a (random) sample of 1000 individuals

  \item \blueem{One question:} Total annual household income

    \item \blueem{Often:}  Many survey recipients \blueem{fail to answer}
      some or all questions 

  \end{itemize}  
\end{frame}

\begin{frame}
  \frametitle{Nonresponse in sample surveys}

\redbf{Median annual household income:}  From US Census Bureau, 2023
\begin{itemize}
\item \$80,610 
\item \blueem{Gini coefficient} = 0.485; a measure of statistical
  dispersion often used to reflect the \blueem{extent of income
    inequality}
\item Gini coefficient = 0 $ \Longrightarrow$ perfect equality
  (all members of population have the same income)

\item Gini coefficient = 1 $ \Longrightarrow$ maximal inequality
  (one household has all the income)
    
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Nonresponse in sample surveys}

  \redbf{True distribution of incomes:}  Density with
  these features
  \vspace*{-0.15in}
  \begin{center}
    \includegraphics[width=2.2in,alt={True probability density of
      household incomes in \$10,000s; the density is very right
      skewed, with a right tail of high incomes}]{incomedensity.png}
    \end{center}
\vspace*{-0.15in}
    \begin{itemize}
    \item \blueem{Extremely right skewed}

    \item The analysts conducting the survey \blueem{do not know}
      this true distribution and wish to learn about it
      \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Nonresponse in sample surveys}

  \redbf{``Perfect survey:''}  All 1000 randomly chosen individuals
  \blueem{respond} to the annual household income question
  \begin{itemize}
\item Expect \blueem{histogram} of 1000 responses to be similar to
the true density (left panel on Slide~\pageref{slide:surveypics})

\item \blueem{Sample median and mean (SD)}: \$79,859 and \$119,128
  (\$148,223), with \blueem{red dashed line} showing the \blueem{normal density}
  with this mean and SD

    \item \blueem{Extreme skewness} $ \Longrightarrow$ sample
      mean $>>$ sample median (clearly not normal)
    \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Nonresponse in sample surveys}

  \redbf{ ``Actual survey:''}  Only \blueem{54.7\% (547)} of the 1000
  individuals responded
  \begin{itemize}
\item \blueem{Histogram} of the \blueem{available} responses is
  \blueem{very different} from the true density (right panel on Slide~\pageref{slide:surveypics})

  \item \blueem{Sample median and mean (SD)}: \$56,672 and \$53,168
  (\$32,209), with \blueem{red dashed line} showing the \blueem{normal density}
  with this mean and SD
  
  \item Sample mean and median are \blueem{similar} $
    \Longrightarrow$  normal distribution appears to be a reasonable
    description of the distribution of incomes \blueem{among respondents}

 \end{itemize}   

 
  \end{frame}  

\begin{frame}
  \frametitle{Nonresponse in sample surveys}
 \phantomsection\label{slide:surveypics}

  \begin{center}
    \includegraphics[width=2.15in,alt={Histogram of income data from
      perfect survey, with the true density and a normal density
      with the same mean and standard deviation
      superimposed}]{incomefulldata.png} \hspace{0.05in}
      \includegraphics[width=2.15in,alt={Histogram of income data from
        the actual survey, with the true density and a normal density
      with the same mean and standard deviation
      superimposed}]{incomeobsdata.png}
\end{center}

\blueem{Red dashed line} is normal density with mean/SD = sample mean/SD

\end{frame}  

\begin{frame}
  \frametitle{Nonresponse in sample surveys}

  \redbf{What can we say about the true distribution of incomes in
    the population based on only the actual survey results?}

  \vspace*{0.1in}

  \redbf{Approach (i):} Assume there is \blueem{no difference} in
  the distribution of income between individuals who respond to the
  survey and those who donâ€™t
  \begin{itemize}
\item I.e., the probability that an individual responds \blueem{does not
  depend} on his/her income
    
\item $ \Longrightarrow$ View the 547 responses as a \blueem{
    random (representative) sample} from the population (albeit
  smaller than we'd hoped)

  \item And conclude that the distribution of annual household incomes
    in the population is approximately normal with median/mean
    $\approx$ \$55,000

    \item\blueem{Of course, we have no way of knowing if this is a
      reasonable assumption}
    \end{itemize}

  \end{frame}

  \begin{frame}
  \frametitle{Nonresponse in sample surveys}

  \redbf{What can we say about the true distribution of incomes in
    the population based on only the actual survey results?}

  \vspace*{0.1in}

  \redbf{Approach (ii):} Assume that individuals with higher incomes
  are \blueem{less likely} to divulge their income amounts than those with
  lower income
  \begin{itemize}
  \item E.g., the \blueem{probability of nonresponse increases with income} and
    in fact rises dramatically for incomes larger than \$100,000
    
\item $ \Longrightarrow$  The 547 responses \blueem{are not a
    random (representative) sample} from the population (those with
  lower incomes are \blueem{overrepresented}\,)

\item And thus the sample mean/median \blueem{are not} believable estimates of the true
  mean/median in the population

\item To obtain realistic estimates, need a way of \blueem{
    incorporating this assumption} in an analysis of the actual survey
  data

    \item\blueem{Of course, we have no way of knowing if this is a
      reasonable assumption}
    \end{itemize}

  \end{frame}  

  \begin{frame}
  \frametitle{Nonresponse in sample surveys}

  \redbf{Important takeaways:}
\begin{itemize}
\item Different assumptions about the nature of nonresponse,
  i.e., \blueem{probability of nonresponse given income}, will lead to different
  inferences about the true distribution

\item Assumptions about the nature of nonresponse are \blueem{unverifiable
  from the available data} because we have no information in the data
about the distribution of incomes among nonrespondents

  \end{itemize}
  \end{frame}  

\begin{frame}
  \frametitle{Formalize}

  \redbf{Define:}
  \begin{itemize}
  \item $Y$ = random variable denoting household income for an
    individual in the population

    \item $R$ = random variable taking the value 1 if $Y$ is observed for the
      individual and 0 if it is not

\item $p_{Y|R}( y |r )$ = density of income $Y$ conditional
  on $R$

  \item $p_{R|Y}(r|y)$ = density of $R$ conditional on $Y$

\item $p_R(r)$ = marginal density of $R$    
\end{itemize}

\end{frame}  

\begin{frame}
  \frametitle{Formalize}

\redbf{For each approach:}
\begin{itemize}
\item[ (i)]  Assuming \blueem{no difference in distribution of income} between
  respondents and nonrespondents is equivalent to assuming
\begin{equation}
p_{Y|R}( y| R=1) = p_{Y|R}( y | R=0) \,\,\, \mbox{  i.e.,  }\,\, Y \independent R
\label{eq:one.1}
\end{equation}
or equivalently  
$$p_{R|Y}(r|y) \,\,  \mbox{ does not depend on } \,\,y; \mbox{ i.e., }
\,\, p_{R|Y}(r|y) = p_R(r)$$

\item[(ii)] Assuming that the \blueem{probability of nonresponse
  increases with income} $ \Longrightarrow$ $Y$ and $R$ are
  not independent and 
  $$p_{R|Y}(r|y) \,\,  \mbox{ depends on } \,\, y$$

  \item  \blueem{In either case:} The form of $p_{R|Y}(r|y)$
    \blueem{cannot be verified} from the available data
\end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{Dropout in a clinical trial}

  \redbf{Randomized clinical trial:}  Two treatments coded 0 and 1
\begin{itemize}
 \item \blueem{Goal:}  Compare 0 and 1 on the basis of a health
   outcome of interest

  \item  Subjects are \blueem{randomly assigned} at \blueem{
      baseline} (time $t_1 = 0$) to receive 0 or 1
\item Subjects are \blueem{followed} for a specified period of
  time

\item The outcome and other health status information is ascertained
  at baseline on each, \blueem{prior} to starting assigned treatment

\item Subjects return to the clinic at times $t_2 < \cdots < t_T$ at
  which the outcome and other health status information is ascertained
\end{itemize}

\vspace*{0.1in}

\redbf{Common challenge:} Participants \blueem{drop out}
\begin{itemize}
\item I.e., a subject \blueem{fails to show up} for clinic visits after a
  certain point

\item $ \Longrightarrow$ Outcome, other info \blueem{
    cannot be obtained} after that point
 \end{itemize} 


\end{frame}

\begin{frame}
  \frametitle{Dropout in a clinical trial}

  \redbf{Define:}  For $j=1,\ldots,T$
  \begin{itemize}
\item $Y_j$ = outcome that is to be collected at time $t_j$

\item $V_j$ = additional health status information collected at $t_j$
  \end{itemize}

  \vspace*{0.2in}

  \redbf{Drop out:}  A subject is said to \blueem{drop out} at time $t_j$
if s/he appeared at the clinic at times $t_1, t_2, \ldots, t_{j-1}$
but then ceased to appear at time $t_j$ and onward
\begin{itemize}
  \item If a subject is a \blueem{dropout} at $t_j$, $j \geq 2$,
    $(Y_1,V_1), \ldots, (Y_{j-1},V_{j-1})$ are observed, but $(Y_j,
    V_j), \ldots, (Y_T,V_T)$ are \blueem{missing}

  \item \blueem{Here:} Assume all subjects are observed at \blueem{
      baseline}, so $(Y_1,V_1)$, is available on all
    
   \end{itemize} 

 \end{frame}  

\begin{frame}
  \frametitle{Dropout in a clinical trial}

  \redbf{ Questions of interest:}  Comparison between treatments 0
  and 1 of
  \begin{itemize}
\item Mean outcome at the final follow-up time $t_T$

\item The \blueem{pattern of change} of mean outcome over the study period

\item We focus on the latter
\end{itemize}

  \vspace*{0.1in}

  \redbf{Approach:}  Posit a \blueem{model} for mean outcome
  \begin{itemize}
    \item E.g., if mean trajectories are expected to be \blueem{linear}
  \begin{equation}
E(Y_j) = \beta_{0,a} + \beta_{1,a}\, t_j, \hspace*{0.1in} a = 0, 1
\label{eq:one.2}
\end{equation}

\item $ \Longrightarrow$ Difference in \blueem{slopes} $\beta_{1,1}-\beta_{1,0}$

\end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dropout in a clinical trial}

\redbf{With no dropout:}  $Y_1,\ldots,Y_T$ are available on 
on each study participant at times $t_1,\ldots,t_T$ (along with
treatment assignments)
\begin{itemize}
\item $ \Longrightarrow$ Standard methods for fitting the
  \blueem{population average model} (\ref{eq:one.2}) to the
  longitudinal data yield \blueem{estimators} for $\beta_{1,1}$ and
  $\beta_{1,0}$ that can be used to obtain estimates of 
  for $\beta_{1,1}$ and $\beta_{1,0}$ and their difference

\item If the \blueem{true mean trajectory} really is a \blueem{
    straight line} as in (\ref{eq:one.2}), so that the model is
  \blueem{correctly specified}

  \item Then there are \blueem{true values} of the slopes
    $\beta_{1,1}$ and $\beta_{1,0}$

  \item And the estimators are \blueem{consistent} estimators for
    these values
  
\end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Dropout in a clinical trial}

\redbf{Effect of dropout:}  Consider treatment 1 and inference
on $\beta_{1,1}$
\begin{itemize}
\item Suppose the model (\ref{eq:one.2}) is \blueem{correctly specified}

\item So that there are \blueem{true values} of the slopes
  $\beta_{1,1}$ and $\beta_{1,0}$ 

\item And $Y_j$ is \blueem{normally distributed} with the \blueem{
    same variance} $\var(Y_j) = \sigma^2$ for $j = 1,\ldots,T$

    \item This situation is depicted on Slide~\pageref{slide:overtime}
\end{itemize}

\vspace*{0.2in}

\redbf{Consider three possible scenarios \ldots}

   \end{frame}  

\begin{frame}
  \frametitle{Dropout in a clinical trial}
\phantomsection\label{slide:overtime}
\begin{center}

  \includegraphics[width=3.25in,alt={Plot of mean income over time,
    which follows a straight, increasing line, with normal
    distributions of income at each of several time points}]{dropout_full.png}

\end{center}

 \end{frame}  

\begin{frame}
  \frametitle{Dropout in a clinical trial}

  \redbf{Scenario 1 - Dropout is unrelated to outcome:}
  \begin{itemize}
  \item Subjects drop out because they move to other states for work
    or family considerations for reasons that have \blueem{nothing
      to do} with their health status

  \item $ \Longrightarrow$ Reasonable to believe that at each
    $t_j$ the distribution of outcomes among dropouts is \blueem{the
      same} as that for subjects who continue in the study

  \item \blueem{Result:}  The number of subjects at each time point
    is smaller than intended, but they are still \blueem{
      representative} of the population

  \item So that the standard estimator for $\beta_{1,1}$ is still
    \blueem{consistent}  but with \blueem{lower precision} than hoped

  \end{itemize}  

 \end{frame}  

\begin{frame}
  \frametitle{Dropout in a clinical trial}

\redbf{Scenario 2 - Dropout is related only to information that is
  observed:}
\begin{itemize}

\item Subjects who drop out at any time $t_j$, $j \geq 2$, do so based
  on the \blueem{histories} of their outcome ($Y_1,\ldots,Y_{j-1}$) and other
  information $(V_1,\ldots,V_{j-1})$ being recorded in the study

\item A subject's physician may decide after review of his/her
  evolving outcome and other information to \blueem{remove him/her
    from the study} and place him/her on another treatment

\item E.g., in a trial comparing anti-hypertensive agents with
  systolic blood pressure as the outcome, this decision may be based
  on the subject's blood pressure readings and also on
  his/her heart rate, cholesterol levels, and other health status
  information
 
  \end{itemize}
\end{frame}  

\begin{frame}
  \frametitle{Dropout in a clinical trial}

\redbf{Scenario 2 - Dropout is related only to information that is
  observed:}
\begin{itemize}
\item $ \Longrightarrow$ \blueem{Not reasonable} to believe that
  at each $t_j$ the distribution of outcomes among dropouts is
  \blueem{the same} as that for subjects who continue in the study

  \item Subjects with certain histories may be \blueem{more likely}
    to dropout, and future outcomes may be \blueem{correlated} with history

  \item \blueem{However:} for subjects with \blueem{identical histories
      through time $t_{j-1}$}, some drop out and some do not
  \item So, \blueem{conditional} on history, whether or not a
    subject drops out is essentially \blueem{at random}

  \item $ \Longrightarrow$ The distribution of outcomes at $t_j$
    among dropouts is \blueem{the same} as that for subjects who
    continue in the study \blueem{conditional on history}
    $\{(Y_1,V_1),\ldots,(Y_{j-1},V_{j-1})\}$
    \end{itemize}

 \end{frame}  

\begin{frame}
  \frametitle{Dropout in a clinical trial}

 \redbf{Will the standard analysis applied to the observed data
      result in valid inferences on $\beta_{1,1}$ in this case?}

    \vspace*{0.2in}

    \redbf{Simple situation:} Decision to drop out at $t_j$ depends
    only on a subject's \blueem{last observed outcome}; in particular
    \begin{itemize}
    \item A subject whose observed outcome $Y_{j-1}$ at $t_{j-1}$
      is $>$ the \blueem{mean outcome at $t_{j-1}$ + 1 standard
      deviation (SD)} will drop out of the study with probability
      0.9 and continue with probability 0.1

    \item A subject whose observed outcome $Y_{j-1}$ at $t_{j-1}$ is
      $\leq$ mean outcome at $t_{j-1}$ + 1 SD \blueem{does not drop
        out}

    \item $\Longrightarrow$ At baseline $t_1$, the distribution of
      outcomes in the population is \blueem{normal} as in the left
      panel of   Slide~\pageref{slide:normalMAR}


      \item And subjects with $Y_1$ in the \blueem{blue region} will
        drop out at $t_2$ with probability 0.9
      \end{itemize}
    \end{frame}

    \begin{frame}
  \frametitle{Dropout in a clinical trial}

\redbf{Simple situation, continued:}
\begin{itemize}

\item As before, repeated outcomes on the same subject are \blueem{
    correlated}

  \item E.g., if a subject's outcome is ``high'' at $t_{j-1}$, it is 
    ``high'' at $t_j$.

\item $ \Longrightarrow$ Likely that outcomes at $t_2$ for
  dropouts \blueem{would have also been $>$} mean + 1 SD at $t_2$.  

\item The distribution of \blueem{observed outcomes of non-dropouts}
  at $t_2$ will look like that in the right panel of
  Slide~\pageref{slide:normalMAR},  \blueem{left skewed}

  \item Different from the distribution of \blueem{all outcomes in
      the population at $t_2$}, which is normal

    \item Mean of the distribution of observed outcomes will be
      \blueem{smaller} than the mean of the distribution of
      \blueem{outcomes}
  
\end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Dropout in a clinical trial}
\phantomsection\label{slide:normalMAR}

\vspace*{-0.15in}
\begin{center}
\includegraphics[width=2.15in,alt={Normal density of incomes at
  baseline, with incomes that are 1 standard deviation higher than the
  mean shaded in blue}]{dropout_norm.png} \hspace{0.05in}
\includegraphics[width=2.15in,alt={Apparent density of the incomes for
  individuals who did not dropout, which is left-skewed}]{dropout_skew.png}
\end{center}

\redbf{Result:}  Estimated mean outcome at $t_2$ based on
\blueem{observed data} will be \blueem{smaller than} the true mean
of \blueem{all outcomes in the population} at $t_2$
\end{frame}

\begin{frame}
  \frametitle{Dropout in a clinical trial}

\redbf{Simple situation, continued:}
\begin{itemize}

\item This reasoning \blueem{extends} to  dropout at $t_3,\ldots,t_T$

\item $ \Longrightarrow$ At each time, the distribution of the
  \blueem{observed outcomes} will appear \blueem{more and more
    left skewed} as subjects with observed outcomes at the previous
    time $>$ mean + 1 SD would be highly likely to drop out

\item And the \blueem{estimated mean outcome at each time} based on
  the observed data will be \blueem{smaller and smaller} relative to the true
  mean of the (normal) distribution of all outcomes
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Dropout in a clinical trial}

  \vspace*{-0.15in}
\begin{center}
\includegraphics[width=2.25in,alt={Solid blue line showing the true mean
  outcome over time, red dotted line showing the estimate of the true
  mean outcome from the observed data}]{dropout_both.png}
\end{center}

\vspace*{-0.1in}
\redbf{Result:} The apparent relationship of mean outcome to time
based on the observed data will be \blueem{attenuated} relative to the
true relationship
\begin{itemize}
\item $ \Longrightarrow$ the standard estimator for $\beta_{1,1}$
  will be \blueem{inconsistent} (biased downward)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Dropout in a clinical trial}

\redbf{Scenario 3 - Dropout is related to information that is
  not observed:}
\begin{itemize}
\item Subjects who drop out at any time $t_j$, $j \geq 2$, do so based
  on their \blueem{current outcome $Y_j$}\,, which would then
  \blueem{not} be observed

\item E.g., Subjects with \blueem{higher} $Y_j$ are \blueem{more
    likely to drop out} at $t_j$; a subject takes his blood pressure
    right before a scheduled clinic visit and, if it is high decides
    to quite the study

  \item $ \Longrightarrow$ Dropout is related to information that
    is \blueem{not observed} in that it\blueem{will not
      be recorded in the database available to the analyst}

  \item The distribution of observed outcomes
    will be \blueem{more and more left skewed} over time, estimation
    of $\beta_{1,1}$ will be \blueem{compromised} 
  \end{itemize}

  \vspace*{0.15in}

  \redbf{Thought:}  If information implicated in dropout \blueem{
    is observed}\,, may be hope of ``correcting'' for dropout; if
  \blueem{not observed}, seems hopeless!
\end{frame}

\begin{frame}
  \frametitle{Formalize}

\redbf{As before:} If a subject has not dropped out by $t_j$, 
observe $(Y_j, V_j)$; if a subject drops out at $t_j$, $(Y_j,
V_j)$ and all subsequent pairs are \blueem{missing}

\vspace*{0.1in}

\redbf{Define:} $R = (R_1,\ldots, R_T)$, elements
correspond to each time point 
\begin{eqnarray*}
R_j & = & 1 \,\,\,\mbox{  if $(Y_j,V_j)$ is observed} \\
     & = & 0 \,\,\,\mbox{  if $(Y_j,V_j)$ is missing}, \hspace{0.15in}j = 1,\ldots,T
\end{eqnarray*}
\vspace*{-0.1in}
\begin{itemize}
\item \blueem{All subjects observed} at baseline $ \Longrightarrow$ $R_1=1$

  \item Dropout implies $R$ takes on only values 
$r^{(j)}$, $j=1,\ldots,T$
\begin{equation}
\hspace*{-0.25in}r^{(1)} = (1,0,\ldots,0), \hspace{0.15in} r^{(2)} = (1,1,0,\ldots,0), \hspace{0.15in}\ldots,
\hspace{0.15in} r^{(T)} = (1,1,\ldots,1)
\label{eq:one.2.5}
\end{equation}
\item I.e., $r^{(j)}$ represents dropout at the $(j+1)th$ time point (so
  the subject is last seen at the $j$th time)

\item ``Dropout at time $(T+1)$'' corresponds to \blueem{never
    dropping out} and completing all $T$ clinic visits
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Formalize}

  \redbf{Define:} $Y = (Y_1,\ldots,Y_T)^T$, $V = (V^T_1,\ldots,V^T_T)^T$
  \begin{itemize}
\item $Y$ and $V$ are all the data \blueem{intended to be
    collected}

\item  I.e., $Z = (Y, V)$ are the  \blueem{full data}
    
  \item \blueem{In principle:} Probability of dropping out could
    depend on any aspect of $Y$ and $V$
  \end{itemize}

  \vspace*{0.3in}

  \redbf{In this framework:} Can characterize Scenarios 1 - 3
  through the \blueem{probability of observing dropout pattern $r^{(j)}$,
  $j = 1\ldots,T$}, in (\ref{eq:one.2.5})
\end{frame}


\begin{frame}
  \frametitle{Formalize}

\redbf{Scenario 1 - Dropout is unrelated to outcome:} 
$$\pr(R=r^{(j)} \,|\, Y,V) = \pr(R=r^{(j)})$$

\redbf{Scenario 2 - Dropout is related only to information that is
  observed under $r^{(j)}$:} 
$$\pr(R=r^{(j)} \,|\, Y,V) = \pr(R=r^{(j)} \,|\,Y_1,\ldots,Y_j,V_1,\ldots,V_j)$$

\redbf{Scenario 3 - Dropout is related to information that is
  not observed under $r^{(j)}$:} 
$$\pr(R=r^{(j)} \,|\, Y,V) \,\,\mbox{ depends on }\,\,
Y_{j+1},\ldots,Y_T,V_{j+1},\ldots,V_T$$
\end{frame}


\begin{frame}
  \frametitle{Formalize}

  \redbf{Shortly:} These scenarios can be characterized in terms of
  \blueem{widely used terminology}
  \begin{itemize}
  \item Scenario 1 corresponds to data \blueem{missing completely at random}

  \item Scenario 2 corresponds to data \blueem{missing at random}

    \item Scenario 3 corresponds to data \blueem{missing not at random}
    \end{itemize}

\end{frame}

\subsubsection{General framework and taxonomy of missing data
    mechanisms}

\begin{frame}
  \frametitle{1.  Introduction and Motivation}
  %\label{s:intro}

 
  \redbf{1.3. General framework and taxonomy of missing data
    mechanisms}
  
\end{frame}

 
  \begin{frame}
    \frametitle{Objective}

    \redbf{Basic situation:}
    \begin{itemize}
    \item There is a \blueem{scientific question} pertaining to a
      population of interest

      \item  The question can be posed in terms of the distribution of
        \blueem{random variables} representing information that
        could be collected on individuals in the population

      \item \blueem{Ideally:} A \blueem{sample} of $N$ individuals
        is drawn from the population, and the values of all of the
        variables are recorded on each

      \item Individuals could be subjects in a clinical trial,
        households in a survey, plots in an agricultural experiment, etc

       \item\blueem{If all of the intended variables are observed for all
         individuals}, the question can be addressed with established
       statistical methods based on these data
       \end{itemize}
    \end{frame}
  

\begin{frame}
  \frametitle{Notation}

  \redbf{In general:} $Z$ denotes the \blueem{full data} intended
  to be collected on each individual, which can be partitioned into
  $K$ possibly vector-valued components
\begin{equation}
Z = (Z_1,\ldots,Z_K)
\label{eq:one.3}
\end{equation}
\vspace*{-0.1in}
\begin{itemize}
\item \blueem{Assume:} The elements of each component are either
  \blueem{all missing or all observed}
\end{itemize}

\vspace*{0.1in}

\redbf{Missingness indicators:} \blueem{Indicators} of whether or
not corresponding components of $Z$ are observed
\begin{equation}
R = (R_1,\ldots,R_K)
\label{eq:one.4}
\end{equation}
\vspace*{-0.15in}
\begin{itemize}
 \item I.e., for  $k=1,\ldots,K$
\begin{eqnarray*}
R_k & = & 1 \hspace*{0.1in} \mbox{if $Z_k$ is observed} \\
& = & 0 \hspace*{0.1in} \mbox{if $Z_k$ is missing}
\end{eqnarray*}
   \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Notation}

\redbf{Missingness indicators:} 
\begin{itemize}
\item \blueem{Any of the components} of $Z$ (\ref{eq:one.3}) can be
  observed or missing

\item $ \Longrightarrow$ $R$ in (\ref{eq:one.4}) can take on $2^K$
  \blueem{possible values}

  \item Denote a possible value of $R$ as $r = (r_1,\ldots,r_K)$,
    where $r_k = 0$ or 1 as $Z_k$ is missing or observed

\item \blueem{Terminology:}  Each value of $r$ characterizes a
  \blueem{missingness pattern}

    
  \item \blueem{Define for convenience:}
    \begin{equation}
\Rbar = (1-R_1,\ldots, 1-R_K),
\label{eq:one.Rbar}
\end{equation}
Vector of indicators of whether or not the corresponding components of
$Z$ are \blueem{missing}, possible values $\rbar$

  \end{itemize}
 
 \end{frame}  

 \begin{frame}
   \frametitle{Special case: Regression}

\redbf{Regression:} $Y$ = scalar outcome, $X$ = $p$-dimensional
vector of covariates
\begin{itemize}
\item \blueem{Interest:}  Estimation of parameter $\beta$ in a
  \blueem{parametric model} $\mu(x; \beta)$ for $E(Y| X=x)$

\item If $Y$ can be missing, and components of $X$ are either all
  observed or all missing \\ $ \Longrightarrow$ $K=2$
  \begin{equation}
Z = (Z_1,Z_2) = (Y,X)
\label{eq:one.5}
\end{equation}
\item In (\ref{eq:one.5}), $2^2 = 4$ possible values $r$: $(1,1)$,
  $(1,0)$, $(0,1)$, $(0,0)$

\item If only $Y$ can be missing, 2 possible values $r$:
  $(1,1)$, $(0,1)$

\item If instead each component of $X$ can be observed or missing
  \\ $ \Longrightarrow$ $K=p+1$
  $$Z=(Z_1,Z_2,\ldots,Z_{p+1}) = (Y,X_1,\ldots,X_p)$$
  \end{itemize}
   \end{frame}


   \begin{frame}
   \frametitle{Special case: Regression}

\redbf{Auxiliary data:}  Interest is in model $\mu(x; \beta)$ for $E(Y| X=x)$
\begin{itemize}
\item \blueem{Additional information} $V$ $(q \times 1)$ also collected

  \item Although $V$ is not relevant to the question of interest
    (relationship between $Y$ and $X$), $V$ \blueem{may be useful}
for  rendering assumptions about the mechanism of
  missingness \blueem{plausible}

\item If $Y$ can be missing and $X$ and $V$ are each either all observed or
  all missing $ \Longrightarrow$  $K=3$
$$Z=(Z_1,Z_2,Z_3) = (Y,X,V)$$
\item If instead each component of $(X, V)$ can be missing or not
  $ \Longrightarrow$ $K = 1+p+q$
\end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Special case: Longitudinal study}

   \redbf{Observations over time:} $T$ pre-specified times
   $t_1< \cdots <t_T$ at which observations are to be taken on each
   participant; $t_1$ is baseline
   \begin{itemize}
\item \blueem{At $t_j$:}  Scalar outcome $Y_j$, vector of covariates
  of interest $X_j$,  vector of auxiliary variables $V_j$
\item If subject shows up at $t_j$, $(Y_j,X_j,V_j)$ observed; if
  \blueem{not}, $(Y_j,X_j,V_j)$ missing $ \Longrightarrow$
  $K=T$, $R = (R_1,\ldots,R_T)$
$$Z = (Z_1,\ldots,Z_T) = \{ (Y_1,X_1,V_1), (Y_2,X_2,V_2),\ldots,
(Y_T,X_T,V_T)\}$$

\item \blueem{Monotone missingness:} Missingness is due to \blueem{
    dropout}\,; if all individuals are observed at baseline, as in
  (\ref{eq:one.2.5}) $r$ takes values
$$(1,0,\ldots,0), \hspace{0.15in} (1,1,0,\ldots,0), \hspace{0.15in}\ldots,
\hspace{0.15in} (1,1,\ldots,1)$$

\item \blueem{Nonmonotone missingness:} Missingness is \blueem{
    intermittent}\,; e.g., if $T=5$, $r = (1,1,0,0,1)$ 
  $ \Longrightarrow$  $2^T$ possible patterns

     \end{itemize}

 \end{frame}

 \begin{frame}
   \frametitle{Observed data}

   \redbf{Notation:} \blueem{For a specific pattern of missingness $r$}
   \begin{itemize}
   \item $\Zr$ = subset of components of $Z$ that is \blueem{
       observed} under the pattern $r$

     \item $\Zrbar$ = subset that is \blueem{missing}

   \item E.g., $K=3$, $Z=(Z_1,Z_2,Z_3)$, with $r =
     (1,0,1)$
     $$\Zr = (Z_1,Z_3), \hspace*{0.15in} \Zrbar = Z_2$$ 
   \end{itemize}

\end{frame}

 \begin{frame}
   \frametitle{Observed data}
   
   \redbf{Observed data:} \blueem{Formal definition}
\begin{itemize}
 \item  The available data when some components of $Z$ are missing are 
   \begin{equation}
(R, \ZR), \hspace*{0.20in} \mbox{ where } \hspace*{0.20in}\ZR = \sum_r \Zr \, I(R=r)
\label{eq:one.observed}
\end{equation}

\item Because $R$ is a \blueem{random vector}, $\ZR$
  is \blueem{not the same} as $\Zr$ for a fixed value $r$

\item \blueem{Critical:} $\ZR$ depends on $R$, and thus \blueem{both $R$ and
  $\ZR$ are necessary} to characterize which components of $Z$
are observed for a randomly chosen individual

\item \blueem{Thus:}   (\ref{eq:one.observed}) is referred to as the
  \blueem{observed data}
\end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Observed data}

\redbf{Popular but imprecise notation:}  Some in the missing data
literature refer to components of $Z$ that may be observed and missing by 
rearranging and partitioning $Z$ as
$$Z = (\Zobs,\Zmis)$$
\vspace*{-0.15in}
\begin{itemize}
\item  $\Zobs$ comprises the components of $Z$ that are observed
\item $\Zmis$ comprises those that are missing
\item \blueem{Critical:}  $\Zobs$ and $\Zmis$ are \blueem{not}
of fixed dimension (\blueem{not explicit} with this notation)

\item \blueem{Loosely speaking:}
  $$\Zobs = Z_{(R)} \hspace*{0.1in} \mbox{  and  } \hspace*{0.1in} \Zmis=\ZRbar$$

\item \blueem{However:} It is \blueem{incorrect} to refer to
  $\Zobs$ as the observed data

\item $ \Longrightarrow$ Both $R$ and $\ZR$ are required to
  characterize fully the \blueem{observed data} on a randomly chosen
  individual, as in (\ref{eq:one.observed})
  \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Dropout}

\redbf{Popular notation:}  In a longitudinal study where missingness
arises due to \blueem{dropout}
\begin{itemize}
\item \blueem{Full data:}   $Z = (Z_1,\ldots,Z_T)$ at $T$
  pre-specified times  $t_1 < \cdots < t_T$
\item \blueem{Define:}  Time of dropout
  \vspace*{-0.1in}
\begin{equation}
D = 1 + \sum^T_{j=1} R_j
\label{eq:one.dropout}
\end{equation}
\item $D = j$ $ \Longrightarrow$ subject is \blueem{
    observed} at  $t_1,\ldots,t_{j-1}$ (so that $R_1= \cdots =
R_{j-1}=1$) and is a \blueem{dropout} at $t_j$ ($R_j= \cdots = R_T = 0$)

\item $D = T+1$ $ \Longrightarrow$ \blueem{full data} are observed

\item \blueem{For given $j$:} Modified notation
  $Z_{(j)} = (Z_1,\ldots,Z_{j-1})$ denotes components of $Z$ observed
  under dropout at time $j$

  \item \blueem{Observed data:}  $(D, Z_{(D)})$, \hspace{0.1in}$Z_{(D)} =
    \sum_{j=2}^{T+1} Z_{(j)} I(D = j)$
  \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Sample data}

\redbf{Sample of $N$ (randomly sampled) individuals from the
  population:} Index individuals by $i = 1,\ldots, N$
\begin{itemize}
  \item  \blueem{Full data on individual $i$:}  $Z_i = (Z_{i1},  Z_{i2},\ldots,Z_{iK})$

\item \blueem{Full data for sample:} \blueem{Independent and
    identically distributed} (iid) $Z_i$, $i=1,\ldots,N$ 

\item \blueem{Shorthand:} $\Ztil = \{Z_i, i=1,\ldots,N\}$

\item \blueem{Similarly:} For individual $i$ $R_i$ (analogously
  $D_i$ with dropout)

  \item \blueem{Observed data:}  \blueem{iid} $(R_i, \ZRi)$,
    $i=1,\ldots,N$ (analogously, $(D_i,\ZDi)$ with dropout)

    \item \blueem{Shorthand:} $(\Rtil,\ZRtil)$
  
  \end{itemize}

   \end{frame}

 \begin{frame}
   \frametitle{Inference}

   \redbf{Precise statement of inferential problem:} 
   \begin{itemize}
   \item \blueem{Full data} $Z$ are the data \blueem{intended} to
     be collected

     \item \blueem{Objective:}  Inference on \blueem{some aspect of the
         distribution} of $Z$, e.g., the \blueem{mean} or other characteristic of one
         or more components of $Z$

       \item E.g., \blueem{longitudinal study} with
         $Z = (Y_1,\ldots,Y_T)$ (real-valued outcome $Y$)
         $ \Longrightarrow$ $E(Y_T)$, mean at the \blueem{
           final time point} or \blueem{covariance matrix}  of $(Y_1,\ldots,Y_T)^T$ 
         
   \item Let $p_Z(z)$ = \blueem{probability density} of $Z$    
   \end{itemize}

 \end{frame}

 \begin{frame}
   \frametitle{Inference}

   \redbf{Statistical model:} A \blueem{class of probability
     distributions} assumed to have generated the data and thus 
   assumed to contain the \blueem{true density}

   \vspace*{0.15in}
   

   \redbf{Fully parametric model}: Represent $p_Z(z)$ by a \blueem{
     model}
   $$p_Z(z; \theta)$$
\blueem{indexed by} finite-dimensional vector of parameters
   \begin{itemize}
   \item \blueem{Objective:} Inference on $\theta$ or if 
     $\theta= (\beta^T, \eta^T)^T$ on a subset
     $\beta$ of elements of $\theta$

     \item E.g., components of  $Z = (Z_1,\ldots,Z_K)$ are
  each real-valued random variables $ \Longrightarrow$  $Z$ assumed 
 \blueem{$K$-variate normal} with mean vector $\beta$ and covariance matrix 
 with distinct elements $\eta$

\end{itemize}

\end{frame}

 \begin{frame}
   \frametitle{Inference}

   \redbf{Semiparametric model}: The class of probability
   distributions has densities described by both \blueem{finite-dimensional parametric}
   and \blueem{infinite-dimensional components}
   $$p_Z\{ z;  \beta, \eta(\cdot)\}$$
   \vspace*{-0.15in}
\begin{itemize}
\item Finite-dimensional vector of parameters $\beta$
\item Infinite-dimensional component $\eta(\cdot)$
\item \blueem{Objective:} Inference on $\beta$

\item E.g., $Z=(Y,X)$ with \blueem{assumed parametric model}
  $\mu(x;\beta)$ for $E(Y |X = x)$ with the rest of the density of $Z$
  \blueem{unspecified}  
\end{itemize}

\end{frame}

 \begin{frame}
   \frametitle{Inference}

   \redbf{Given a posited statistical model for the full data:}
   \begin{itemize}
   \item If the \blueem{full data} $Z_i$, $i=1,\ldots,N$, were
     available for all $N$, inference would proceed using methods
     appropriate for the type of full data model adopted and the
     specific goal (e.g., inference on $\beta$)

   \item \blueem{However:} Some components of $Z$ may be
     \blueem{missing}

     \item \blueem{Challenge:} How to achieve valid inference using
     the \blueem{observed data} $(R_i, \ZRi)$, $i=1,\ldots,N$
     \end{itemize}

 \end{frame}

\begin{frame}
   \frametitle{Missing data mechanisms}

\redbf{Taxonomy of missing data mechanisms:}  Given in a
seminal \blueem{Biometrika} paper by Don Rubin  (Rubin, 1976)
\begin{itemize}
\item \blueem{Key principle:}  A particular missingness pattern may
  depend on aspects of the full data $Z$ 
  
\item \blueem{Formal definitions} of mechanisms leading to missing
  data that clarify this dependence
  \end{itemize}
   
    \end{frame}

   \begin{frame}
   \frametitle{Missing data mechanisms}

\redbf{Missing Completely at Random (MCAR):}  Data are 
     \blueem{missing completely at random} if for all missingness
    patterns $r$
\begin{equation}
\pr(R = r | Z) \,\, \mbox{ does not depend on }\,\, Z
\label{eq:one.mcar}
\end{equation}
\vspace*{-0.1in}
\begin{itemize}
\item $ \Longrightarrow$ $R$ and $Z$ are  \blueem{independent}, $R \independent Z$

\item Under MCAR as in (\ref{eq:one.mcar}), the probabilities of the
  possible missingness patterns are \blueem{constants}; i.e.,
$$\pr(R=r|Z) = \pi(r)$$
where $\pi(r)$ is a constant for each possible value $r$

\item \blueem{Plausible} in situation where missingness has
  \blueem{nothing to do with} the issues under study

\item E.g., subjects in in a clinical trial \blueem{move away}

  \item E.g., in the \blueem{surrogate measurement example}\,, if
    the validation sample is chosen by selecting members with
    probability 0.1, missingness of the diary measurement is
    completely at random
   \end{itemize}
   \end{frame}

\begin{frame}
   \frametitle{Missing data mechanisms}

   \redbf{Wrinkle:} In regression example (\ref{eq:one.5}) with
   $Z = (Y, X)$
   \begin{itemize}
     \item Interest in $E(Y|X=x)$ $ \Longrightarrow$ inference is \blueem{conditional} on $X$

\item When $X$ is \blueem{always observed} and only $Y$ can be
  missing $ \Longrightarrow$ \blueem{common convention} to refer
  to 
\begin{equation}
\pr(R = r|Z) = \pr(R = r| Y,X) = \pr(R = r|X) = \pi(r, X),
\label{eq:one.6}
\end{equation}
as \blueem{(conditional) MCAR} $ \Longrightarrow$  $R$ is  independent of $Z$
\blueem{conditional} on $X$, $R \independent Z\, | \, X$

\item Because interest is on aspects of the \blueem{conditional
    distribution} of $Y$ given $X$, this \blueem{conditional independence} is
  analogous to (\ref{eq:one.mcar})

\item \blueem{In this course:} We \blueem{do not} adopt this
  convention and consider (\ref{eq:one.6}) as a special case of
  \blueem{missing at random} (next)
\end{itemize}
   
   
   \end{frame}

   
 \begin{frame}
   \frametitle{Missing data mechanisms}
\redbf{Missing at Random (MAR):}  Data are 
     \blueem{missing at random} if for all missingness patterns $r$
   \begin{equation}
     \pr(R = r | Z)  = \pr(R = r | \Zr) = \pi(r, \Zr)
     \label{eq:one.mar1}
   \end{equation}
   \vspace*{-0.15in}
\begin{itemize}
\item $ \Longrightarrow$ probability of missingness pattern $r$ as a function
of $Z$ depends only on the components of $Z$ that \blueem{are observed}
under $r$

\item \blueem{Dropout in a longitudinal study:} Possible
values of $r$ are  $r^{(j)}$ in (\ref{eq:one.2.5}) $
\Longrightarrow$  $Z_{(r^{(j)})}= (Z_1,\ldots,Z_j)$, $Z_{(\bar{r}^{(j)})}=
(Z_{j+1},\ldots,Z_T)$

\item (\ref{eq:one.mar1}) in terms of  $D$ in (\ref{eq:one.dropout})
\begin{equation*}
\hspace*{-0.2in} \pr(D = j+1 | Z) = \pr( D = j+1 | Z_{(r^{(j)})},Z_{(\bar{r}^{(j)})})  = \pr (D
= j+1 | Z_{(r^{(j)})})
\end{equation*}

\item $ \Longrightarrow$ MAR implies dropout is a \blueem{
    sequentially random} process; whether or not a subject drops out
  at $j$ is at random \blueem{depending only on the history of observed data to that point}

\end{itemize}
   \end{frame}
   
   \begin{frame}
   \frametitle{Missing data mechanisms}
\redbf{Missing at Random (MAR), continued:} 
\begin{itemize}
\item E.g., in the \blueem{surrogate measurement example}\,, if
    the validation sample is chosen by selecting members 
depending on \blueem{whether or not their 24-hour recall measurements
  exceed a threshold} $ \Longrightarrow$ missingness of the diary measurement
\blueem{depends on a variable that is always observed} and thus is MAR


\item \blueem{Intuitively:}  As before, because under MAR missingness depends
  on that are \blueem{always observed}\,, there is hope that the
  observed data could be used to ``\blueem{correct}\,'' for missingness
  \end{itemize}

 \end{frame}

  \begin{frame}
   \frametitle{Missing data mechanisms}
\redbf{Missing Not at Random (MNAR):}  Data are 
     \blueem{missing not at random} if 
   \begin{equation*}
\pr(R = r | Z) \,\, \mbox{ depends on components of $Z$ not
  observed when $R = r$}
\end{equation*}
\vspace*{-0.2in}
\begin{itemize}
\item $ \Longrightarrow$ missingness depends on data that are
  \blueem{not observed} under pattern $r$

\item \blueem{Intuitively:} MNAR is the ``\blueem{worst}\,''
  missingness mechanism, as it \blueem{does not seem possible} to
  use the observed data to ``\blueem{correct}\,'' for missingness
\end{itemize}

\vspace*{0.1in}

\redbf{Taxonomy of missing data mechanisms:} Clarifies the extent to
which missing data \blueem{complicate inference}
\begin{itemize}
\item MCAR is \blueem{least problematic} for valid inference based
  on the observed data

\item MNAR poses a \blueem{significant obstacle} to valid inference

  \item MAR is \blueem{intermediate}\,, valid inference may be possible  

  \end{itemize}

   \end{frame}

  \begin{frame}
   \frametitle{Fundamental challenge}

   \redbf{Challenge of inference when some data are missing:}  
\begin{itemize}
\item It is \blueem{not possible} to test if a particular mechanism
  holds \blueem{based on the observed data}

\item $ \Longrightarrow$ Must adopt an \blueem{assumption} about
  the mechanism \blueem{without being able to verify} its
  plausibility from the data
 
\end{itemize}

\vspace*{0.1in}

\redbf{MNAR versus MAR:} \blueem{Clearly}\,, it is \blueem{not
  possible} to distinguish between MNAR and MAR from the \blueem{observed data
  only}
\begin{itemize}
\item Because MNAR mechanisms depend on data that are \blueem{not
    observed}\,, it is \blueem{impossible} to test if a mechanism
  that depends on data that may be missing is a \blueem{more
    plausible explanation} for missingness than one that
  depends only on observed data

  \item $ \Longrightarrow$ Adopting MAR must be \blueem{defensible}
  on scientific, subject matter, and/or practical grounds, because it
  \blueem{cannot be validated from the data}


  \end{itemize}
\end{frame}

  \begin{frame}
   \frametitle{Fundamental challenge}

\redbf{MAR versus MCAR:}  If one is \blueem{willing to assume} a
MAR mechanism (depends only on \blueem{observed data}\,)
\begin{itemize}
\item It is possible to assess the \blueem{strength of that dependence} from
  the observed data

  \item \blueem{However:}  Is predicated on the MAR assumption, which
    is \blueem{not verifiable} from the observed data

    \item MCAR is generally \blueem{not a realistic assumption} when
      the individuals are \blueem{humans} unless missingness is by design
  \end{itemize}

  \end{frame}

  \begin{frame}
   \frametitle{Fundamental challenge}

  \redbf{Result:} Would \blueem{like to be able to assume} MAR
\begin{itemize}
\item \blueem{Sufficiently rich information}
  that is \blueem{always observed} must be collected and available
  to the analyst

\item E.g., $Z=(Y,X,V)$ and interest in $\beta$ in model
  $\mu(x;\beta)$ for $E(Y|X=x)$, and $Y$ and/or $X$ may be missing

\item 
  $ \Longrightarrow$ \blueem{Collection of additional information $V$ that is always observed and
that may be associated with missingness can render MAR plausible}


\item Guidelines for \blueem{handling missing data in clinical
    trials} published by the National Research Council (2010)
  emphasize \blueem{taking steps at the design stage to prevent
    missing data} and, recognizing that some missing data are
    unavoidable, \blueem{collecting rich information that can be used to
    support MAR}
\end{itemize}
  
   \end{frame}

\subsubsection{More examples}
\label{ss:one-moreexamples}
   
   \begin{frame}
  \frametitle{1.  Introduction and Motivation}
  %\label{s:intro}

 \redbf{1.4. More examples}

\end{frame}

\begin{frame}
   \frametitle{Example 1: Estimation of the mean}

\redbf{Full data:}  $Z = Y$, $Y$ is a real-valued outcome random variable
\begin{itemize}
\item \blueem{Objective:}  Estimate  $\mu = E(Y)$ based on a
random sample of $N$ individuals

\item If full data available on all $N$ individuals $
  \Longrightarrow$ $Y_i$, $i=1,\ldots,N$ iid

\item \blueem{Natural estimator for $\mu$:}  Sample mean
$$\hatmu^{full} = N^{-1} \sumiN Y_i$$
 \end{itemize}

\end{frame}

\begin{frame}
   \frametitle{Example 1: Estimation of the mean}

 \redbf{Missingness:} $Y$ is missing for some individuals, $R=1 (0)$ if
 $Y$ is observed (missing)
 \begin{itemize}
\item \blueem{Observed data:} $(R,\ZR) = (R,RY)$, and observed data
  from $N$ individuals are
  $$(R_i,R_i Y_i), \hspace*{0.1in} i=1,\ldots,N$$  

\item \blueem{Complete case estimator for $\mu$:}
  $$\hatmu^c = \displaystyle{ \frac{\sumiN R_i Y_i}{\sumiN R_i} }$$
depends only on the \blueem{complete cases}, i.e., individuals for
whom $Y$ is observed (and is a \blueem{function of the observed data}\,)

     \end{itemize}

     \vspace*{0.15in}

     \redbf{What are the properties of $\hatmu^c$ under different
       missingness mechanisms?}
   
   \end{frame}

\begin{frame}
   \frametitle{Example 1: Estimation of the mean}

   \redbf{Under MCAR:}  $R \independent Y$, and for $\pi > 0$ a constant
$$\pr(R = 1 | Y) = \pr(R=1) = \pi, \hspace*{0.1in} \pr(R = 0 | Y) =
\pr(R=0) = 1-\pi$$
\vspace*{-0.15in}
\begin{itemize}

  \item  $N \rightarrow \infty$, by the \blueem{weak law of large
      numbers (WLLN)} 
$$\hatmu^c =  \displaystyle \frac{N^{-1} \sumiN R_i Y_i}{ N^{-1} \sumiN R_i} \inp \frac{E(R Y)}{E(R)} =
\frac{E(R) E(Y)}{E(R)} = \mu$$ 
using $R \independent Y$

\item $ \Longrightarrow$ Under MCAR, $\hatmu^c$ is a \blueem{consistent} estimator for
  $\mu$
 
\item \blueem{Aligned with intuition:} Under MCAR,missingness of $Y$
  has \blueem{nothing to do} with $Y$ $ \Longrightarrow$ the
  sample with $Y$ observed is \blueem{smaller than planned but still
    representative of the population}\, so $\hatmu^c$ will yield valid
  but less precise than hoped inference on $\mu$  

    \end{itemize}

   \end{frame}

\begin{frame}
   \frametitle{Example 1: Estimation of the mean}

  \redbf{Under MNAR:} For some $\pi(y) > 0$
  $$\pr(R = 1 | Y) = \pi(Y)$$
\vspace*{-0.15in}
\begin{itemize}

\item  $N \rightarrow \infty$, using $E(R|Y) = \pr(R=1|Y)=\pi(Y)$,
  using WLLN
  \begin{align*}
\hspace*{-0.25in}\hatmu^c =  \displaystyle \frac{N^{-1} \sumiN R_i Y_i}{ N^{-1} \sumiN R_i} \inp& \frac{E(R Y)}{E(R)} 
= \frac{ E\{ \, E(R Y | Y)\, \} } { E\{ \, E(R|Y) \,\}} =  \frac{ E\{
                                                                                                  \,Y \, E(R | Y)\, \} } { E\{ \, E(R|Y) \,\}} \\
    &=  \frac{E\{ Y \, \pi(Y)\} }{E\{ \pi(Y)\}} \neq E(Y) = \mu
    \end{align*}

\item If $\pi(y)$ is \blueem{increasing} in $y$, so that
the probability of observing $Y$ increases with $y$ (check)
$$\frac{ E\{\, Y\, \pi(Y)\} } { E\{ \pi(Y)\}} > \mu$$

\end{itemize}
  
 \end{frame}

 \begin{frame}
   \frametitle{Example 1: Estimation of the mean}

   \redbf{Under MNAR, continued:}
   \begin{itemize}
   \item $ \Longrightarrow$ Under MNAR, $\hatmu^c$ \blueem{need
       not be a consistent estimator} for $\mu$
     
\item \blueem{Note:} Because $Y$ is \blueem{missing} when $R=0$, no way
to \blueem{model and estimate} $\pi(y) = \pr(R=1 | Y=y)$ from the observed
data

\item An \blueem{alternative estimator} that is consistent  seems hopeless

\end{itemize}

\end{frame}

 \begin{frame}
   \frametitle{Example 1: Estimation of the mean}

\redbf{Modified scenario:} \blueem{In addition} to $Y$, set of variables
$V$ is \blueem{collected on all individuals} 
\begin{itemize}
\item \blueem{Full data:} $Z = (Z_1,Z_2) = (Y,V)$

\item $V$ is \blueem{always observed}\,, $Y$ can be missing $
  \Longrightarrow$  $R = (R_1,R_2)$ takes on \blueem{two values}
  $(1,1)$, $(0,1)$

  \item \blueem{Simplify:}  Define $C = 1$ if $R = (1,1)$, $C=0$ if
    $R = (0,1)$ \blueem{complete case indicator}

  \item \blueem{Observed data:}   $(R, \ZR)$ equivalently $(C, CY, V)$

  \item Observed data from $N$ individuals
    $$(C_i, C_i Y_i,V_i), \hspace*{0.1in} i = 1,\ldots,N$$
    \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Example 1: Estimation of the mean}

   \redbf{Willing to assume:} Missingness of $Y$ \blueem{depends only}
on $V$, not on $Y$
\begin{equation}
\pr(C = 1 | Y, V) = \pr(C = 1 | V) = \pi(V), \hspace*{0.15in} \pi(v) >
0\,\, \mbox{ for all } \,\,v
\label{eq:one.ex1.mar}
\end{equation}
$ \Longrightarrow$ $\pr(C = 0 | V) = 1-\pi(V)$ and thus
$C \independent Y | V$ and $R \independent Y| V$
\begin{itemize}
\item Missingness mechanism is \blueem{MAR}\,, depending on data that are
\blueem{always observed}

\item \blueem{Note:}  $V$ may be related to \blueem{
    both} $Y$ and $C$
  $ \Longrightarrow$ if $V$ not available, still possible that
$$\pr( C=1 | Y) \hspace*{0.1in} \mbox{ depends on $Y$}$$

\item \blueem{Thus:}  Collection of additional information $V$
  \blueem{facilitates} assumption of MAR
   \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Example 1: Estimation of the mean}

\redbf{Is $\hatmu^c$ consistent under (\ref{eq:one.ex1.mar})?}  As
$N \rightarrow \infty$, by WLLN
\begin{align*}
\hspace*{-0.1in}\hatmu^c =  \displaystyle \frac{N^{-1} \sumiN C_i Y_i}{ N^{-1} \sumiN C_i} \inp& \frac{E(C Y)}{E(C)} 
= \frac{ E\{ \, E(C Y | Y,V)\, \} } { E\{ \, E(C|Y,V) \,\}} = \frac{
  E\{ \,Y E(C | Y,V)\, \} } { E\{ \, E(C|Y,V) \,\}} \\ &= \frac{E \{ Y
  \pi(V) \} } { E\{ \pi(V) \} }\neq E(Y) = \mu
  \end{align*}
because $Y$ and $V$ can be \blueem{correlated}
\begin{itemize}
\item \blueem{Under MAR:}  The complete case estimator \blueem{is
    not} a consistent estimator for $\mu$ in general

\item \blueem{However:} Because \blueem{$V$ is available}, it is
  possible to construct alternative estimators for $\mu$ that
  \blueem{are consistent} under this form of MAR

  \end{itemize}
   \end{frame}

 \begin{frame}
   \frametitle{Example 1: Estimation of the mean}

   \redbf{Alternative estimator:} For a randomly chosen individual
   with a particular value of $V$, by (\ref{eq:one.ex1.mar}), the
   probability that $Y$ is observed (\blueem{complete case}\,) =
   $\pi(V)$
   \begin{itemize}
\item \blueem{Intuitively:} This individual represents $1/\pi(V)$
randomly chosen individuals, some of whom will have $Y$ \blueem{missing}

\item \blueem{Inverse probability weighting of complete cases:}  
\begin{equation}
\hatmu^{ipw} = N^{-1} \sumiN \frac{C_i Y_i}{\pi(V_i)}.
\label{eq:one.ex1.ipw}
\end{equation}

\item $ \Longrightarrow$  Contributions from complete cases are
  \blueem{weighted} by the reciprocal of the probability of being a
  complete case so \blueem{represent themselves and others like them for whom
  $Y$ may be missing}
 
     \end{itemize}

   
   \end{frame}

\begin{frame}
   \frametitle{Example 1: Estimation of the mean}

\redbf{Is the inverse probability weighted estimator
  (\ref{eq:one.ex1.ipw}) consistent?}  As $N \rightarrow \infty$, by WLLN
\begin{align*}
\hatmu^{ipw} &=  N^{-1} \sumiN \frac{C_i Y_i}{\pi(V_i)} \inp 
E\left\{ \frac{C Y}{\pi(V)} \right\} = E\left[ E\left\{ \left. \frac{C Y}{\pi(V)} \right|  Y,V \right\} \right]\\
&= E\left\{\frac{Y}{\pi(V)} E( C\,| Y,V) \right\}   
= E\left\{\frac{Y}{\pi(V)} \pi(V) \right\} = \mu
\end{align*}
\begin{itemize}
\item $\pi(V)/\pi(V)$ is equal to 1 because $\pi(v) > 0$ for all $v$

  \item \blueem{Thus:}  $\hatmu^{ipw}$ is a \blueem{consistent estimator} for $\mu$

  \end{itemize}
   
   \end{frame}


   \begin{frame}
   \frametitle{Example 1: Estimation of the mean}

   \redbf{Result:}  This argument demonstrates
   \begin{itemize}

\item \blueem{Under MAR:} It is possible to construct estimators based 
on the observed data that ``\blueem{correct}'' for the missingness;
i.e., are \blueem{consistent} estimators for population quantities of interes.

\item \blueem{Inverse probability weighting of complete cases} is one
  general approach (discussed in detail later)

\end{itemize}
 
   
   \end{frame}


   \begin{frame}
   \frametitle{Notational conventions}

\redbf{To facilitate further developments:}  Quick detour to describe
\blueem{notational conventions} needed for discussion of missing
data in \blueem{more complex statistical models}

\vspace*{0.1in}

\redbf{Posited parametric model for $p_Z(z)$:}  $p_Z(z; \theta)$,
$\theta$ $(p \times 1)$
\begin{itemize}
\item For any \blueem{real-valued function} $f(z; \theta)$
  \begin{equation}
\frac{\partial}{\partial \theta} \{ f(z; \theta)\}  \hspace*{0.15in}
(p \times 1) 
\label{eq:one.notation.1}
\end{equation}
\blueem{vector of partial derivatives} of $f(z; \theta)$ with respect
to the components of $\theta$

\item (\ref{eq:one.notation.1}) evaluated at a particular value of $\theta$, $\theta^*$
\begin{equation}
\left. \frac{\partial}{\partial \theta} \{ f(z; \theta)\}
\right|_{\theta=\theta^*}  \,\, \mbox{ in shorthand } \,\, \frac{\partial}{\partial \theta} \{ f(z; \theta^*)\} 
\label{eq:one.notation.2}
\end{equation}

  \end{itemize}
   
   \end{frame}


\begin{frame}
   \frametitle{Notational conventions}

   \redbf{Posited parametric model, continued:}
   \begin{itemize}
     \item \blueem{Result:}  
$$g(z; \theta) = \frac{\partial}{\partial \theta} \{ f(z; \theta)\} \hspace*{0.1in} (p \times 1)$$
refers to the vector function of $\theta$ arising from differentiation
with respect to the second argument as in (\ref{eq:one.notation.1}),
and $g(z;\theta^*)$ is shorthand for (\ref{eq:one.notation.2})

\item \blueem{Similarly:}  $(p \times p)$ \blueem{matrix of second partial derivatives} of 
$f(z; \theta)$ with respect to the components of $\theta$
$$G(z; \theta) = \frac{\partial^2}{\partial \theta \partial \theta^T}
\{ f(z; \theta)\}$$
analogous shorthand
$$\frac{\partial^2}{\partial \theta \partial \theta^T} \{ f(z;
\theta^*)\} \,\,\, \mbox{  and  }\,\,\, G(z; \theta^*)$$


   \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Notational conventions}

   \redbf{Expectations:}  For values $\theta$, $\theta^*$
   $$E_\theta\{ f(Z; \theta^*)\} = \int \, f(z; \theta^*) \, p_Z(z;
   \theta) \, dz$$ Expectation is with respect to the \blueem{density of $Z$
   evaluated at $\theta$}\,, with $f(z;\theta^*)$ is evaluated at 
   \blueem{possibly different} value $\theta^*$
   \begin{itemize}
   \item Write $E_\theta\{ f(z; \theta)\}$ when $\theta = \theta^*$
   \end{itemize}


\vspace*{0.1in}  

\redbf{Correctly specified model $p_Z(z; \theta)$:} If there is a
value $\theta_0$ such that $p_Z(z; \theta_0)$ is the \blueem{true density}
of $Z$, then the model $p_Z(z; \theta)$ is said to be \blueem{correctly
  specified}
\begin{itemize}
\item And $\theta_0$ is referred to as the \blueem{true value of
    $\theta$} in this correctly specified model
  \end{itemize}


 \end{frame}

 \begin{frame}
   \frametitle{Notational conventions}

   \redbf{Expectations, continued:}
   \begin{itemize}
   \item For function $f(Z)$
     $$E\{ f(Z) \} = E_{\theta_0}\{ f(Z) \} = \int \, f(z) \, p_Z(z; \theta_0) \, dz$$
     $ \Longrightarrow$ drop subscript $\theta_0$, when the
     expectation is with respect to the \blueem{true density} of $Z$

   \item For function $f(z;\theta)$ indexed by $\theta$
     $$E\{ f(Z;\theta) \} = E_{\theta_0}\{ f(Z;\theta) \} = \int \, f(z;\theta) \, p_Z(z; \theta_0) \, dz$$
where $\theta$ \blueem{need not} be equal to $\theta_0$
\end{itemize}

\vspace*{0.1in}

\redbf{Henceforth:}  These conventions are used without comment
   
 \end{frame}


   \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

\redbf{Regression situation (\ref{eq:one.5}):}  \blueem{Full data}
$Z= (Y,X)$, $Y$ is a \blueem{scalar outcome}, $X$ is a \blueem{vector of covariates}
\begin{itemize}
\item \blueem{Suppose:}  It is possible for $Y$ to be missing, and
  all components of $X$ are either observed or missing together

  \item $R = (R_1,R_2)$, $R_1 = 1$ if $Y$ observed, = 0 if missing;
    $R_2 = 1$ if $X$ is observed, = 0 if missing

    \item \blueem{Four possible missingness patterns $r$:}
  $$\hspace{-0.25in}r = (1,1), \mbox{ $Y$ and $X$ observed}; \hspace*{0.1in}
r = (1,0), \mbox{ $Y$ observed, $X$ missing}$$
$$\hspace{-0.25in}r = (0,1),  \mbox{ $Y$ missing, $X$ observed}; \hspace*{0.1in}
r = (0,0), \mbox{  $Y$ and $X$ missing}$$

\item \blueem{Define:}  $\pi\{r, (Y, X)\} = \pr(R = r| Y,X)$
 \end{itemize} 
   
 \end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

\redbf{Regression model:}   Interest in inference on $\beta$ in a
model for $E(Y | X = x)$ given by 
$$\mu(x; \beta),  \hspace*{0.15in} \beta \,\,\, (p \times 1)$$
\vspace*{-0.1in}
\begin{itemize}
\item \blueem{No other assumptions} about the distribution of full
  data $Z$   beyond this assumption on the conditional mean

\item $ \Longrightarrow$ \blueem{Semiparametric statistical model}

  \item \blueem{Assume:} The representation $\mu(x; \beta)$ for
    $E(Y | X = x)$ is \blueem{correctly specified} in that there
    exists $\beta_0$ for which $\mu(x; \beta_0)$ coincides with the
    true function $E(Y|X = x)$ of $x$

    \item So that the semiparametric statistical model for the
      distribution of $Z$ is \blueem{correctly specified}

  \end{itemize} 
 \end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

\redbf{Complete case analysis:}  Individuals with $R=(1,1)$
\begin{itemize}
\item \blueem{Full data analysis:}  If the \blueem{full data}
  $$(Y_i, X_i), \hspace{0.1in} i = 1,\ldots,N$$
were available on \blueem{all $N$ individuals} $ \Longrightarrow$ \blueem{
  estimate} $\beta$ using \blueem{ordinary least squares} (OLS) 
  
\item \blueem{Default in most software:} \blueem{Disregard} data
  records where one or more analysis variables are missing and carry
  out the analysis \blueem{based only on the complete cases}

\item \blueem{Observed data from sample:}
  $$(R_i, R_{1i} Y_i, R_{2i} X_i), \hspace{0.1in} i = 1,\ldots,N$$
$ \Longrightarrow$  \blueem{Complete cases} have $R_i = (R_{1i},R_{2i}) = (1,1)$

\item Implications for \blueem{validity of inference} on $\beta$?
  \end{itemize}

\end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

\redbf{OLS estimating equation based on complete cases:}  Solve in $\beta$ 
\begin{equation}
\sumiN I\{ R_i=(1,1)\} \, \frac{\partial}{\partial \beta} \{\mu(X_i;\beta)\} \{Y_i - \mu(X_i;\beta) \}= 0
\label{eq:one.ex2.ols}
\end{equation}

\vspace*{0.1in}

\redbf{Unbiased estimating equation:}  As discussed shortly, if
(\ref{eq:one.ex2.ols}) is such that 
\begin{equation}
E_\beta\left[ \,  I\{ R=(1,1)\} \,  \frac{\partial}{\partial \beta} \{\mu(X ;\beta)\} \{Y - \mu(X;\beta) \}\, \right] = 0
\label{eq:one.ex2.olssolution}
\end{equation}
for all $\beta$, it is an \blueem{unbiased estimating equation}
\begin{itemize}
\item $ \Longrightarrow$ under regularity conditions, if
  (\ref{eq:one.ex2.olssolution}) holds at $\beta=\beta_0$, the
  \blueem{complete case estimator} $\hatbeta^c$ obtained from solving
  (\ref{eq:one.ex2.ols}) satisfies $\hatbeta^c \inp \beta_0$ as
  $N \rightarrow \infty$

  \item \blueem{That is:}   $\hatbeta^c$ is a  \blueem{consistent estimator}

  \end{itemize}

\end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

\redbf{Is (\ref{eq:one.ex2.ols})  unbiased?}  Does (\ref{eq:one.ex2.olssolution})  hold?
\begin{itemize}
  \item Examine under \blueem{different assumptions on the missingness
mechanism}
\end{itemize}

\vspace*{0.15in}

\redbf{Can show (conditioning argument):}
(\ref{eq:one.ex2.olssolution}) can be written as
\begin{equation}
E_\beta\left[ \,  \pi\{(1,1), (Y, X)\} \, \frac{\partial}{\partial \beta}
  \{\mu(X ;\beta)\}  \{Y - \mu(X;\beta) \}\, \right] = 0.
\label{eq:one.ex2.olssolution2}
\end{equation}
\begin{itemize}
\item $ \Longrightarrow$ Examine the \blueem{root} of
  (\ref{eq:one.ex2.olssolution2}) under \blueem{different
    assumptions} on the missingness mechanism

  \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}
\phantomsection\label{slide:case1reg}

   
\redbf{Case 1 - $\pi\{(1,1), (Y, X)\}$ depends only on $X$:}   
$$\pi\{(1,1), (Y, X)\} = \pi\{(1,1),X\}$$
$ \Longrightarrow$ left hand side of
(\ref{eq:one.ex2.olssolution2}) becomes
\begin{align*}
E_\beta&\left[ \,  \pi\{(1,1), X\} \, \frac{\partial}{\partial \beta} \{\mu(X;\beta)\}  \{Y -
    \mu(X;\beta) \}\, \right]  \\
&= E\left( \, E_\beta\left[ \,  \pi\{(1,1), X\} \, \left. \frac{\partial}{\partial \beta} \{\mu(X;\beta)\}  \{Y - \mu(X;\beta) \}\, \right| X \right] \,\right)  \nonumber \\
&= E\left( \, \pi\{(1,1), X\} \, \frac{\partial}{\partial \beta}
  \{\mu(X ;\beta)\}  \, E_\beta\left[ \, \{Y - \mu(X;\beta) \} \mid X \right] \,\right)
\end{align*}
\begin{itemize}
\item $\mu(x;\beta)$ is \blueem{correctly specified} $
  \Longrightarrow$ setting $\beta = \beta_0$ gives
$$E\left[ \, \{Y - \mu(X;\beta_0) \}\, | X \right] = 0$$
\item \blueem{Thus:}  (\ref{eq:one.ex2.olssolution2}) holds, and  $\hatbeta^c$ is \blueem{consistent}
  \end{itemize}
 \end{frame}


 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

\redbf{Case 1 - $\pi\{(1,1), (Y, X)\}$ depends only on $X$:}  On
Slide~\pageref{slide:case1reg}, did not specify the \blueem{missingness
  mechanism}
\begin{itemize}
\item \blueem{Suppose:}  $X$ can be \blueem{missing}\,, $Y$ is
  \blueem{always observed}  $ \Longrightarrow$
$$\pi\{(0,1), X\} = \pi\{(0,0),X\} = 0$$

\item $\sum_r \pi(r, X) = 1$ (sum over 4 possible values of $r$) $ \Longrightarrow$
  $$\pi\{(1,0), X\} = 1 - \pi\{(1,1),  X\} \,\,\,\mbox{ depends on $X$}$$

\item \blueem{Thus:} Probability of observing missingness
  pattern where \blueem{$Y$ is observed but $X$ is missing} depends on
  \blueem{unobserved} $X$ 

  \item So the missingness mechanism is \blueem{MNAR}

\item $ \Longrightarrow$ $\hatbeta^c$ is consistent \blueem{even though the 
missing data are MNAR}
    
  \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

\redbf{Case 1 - $\pi\{(1,1), (Y, X)\}$ depends only on $X$,
  continued:}  
\begin{itemize}
\item \blueem{Suppose:} $Y$ can be \blueem{missing}\,, $X$ is
  \blueem{always observed}  $ \Longrightarrow$
$$\pi\{(1,0), X\} = \pi\{(0,0),X\} = 0$$

\item $\sum_r \pi(r, X) = 1$ $ \Longrightarrow$
  $$\pi\{(0,1), X\} = 1 - \pi\{(1,1),  X\} \,\,\,\mbox{ depends on $X$}$$

\item \blueem{Thus:} Probability of observing missingness
  pattern where \blueem{$X$ is observed but $Y$ is missing} depends
  only on \blueem{always observed} $X$ 

  \item So the missingness mechanism is \blueem{MAR}

\item $ \Longrightarrow$ $\hatbeta^c$ is consistent \blueem{even though the 
missing data are MAR}
 
  \end{itemize}
   
 \end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

\redbf{Result:}  \blueem{Regardless} of whether or not the missingness
mechanism is \blueem{MAR or MNAR}\,, the complete case OLS estimator is 
\blueem{consistent} if $\pi\{(1,1), (Y, X)\}$ depends only on $X$
\begin{itemize}

\item \blueem{However:} Should \blueem{not} conclude 
  that the complete case analysis \blueem{always yields valid
    inferences}

  \item The next case demonstrates this principle
  \end{itemize}

 \end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

   \redbf{Case 2 - $\pi\{(1,1), (Y, X)\}$ depends on $Y$:}
   Probability of observing a \blueem{complete case} depends on
   \blueem{$Y$ alone} or on \blueem{both $Y$ and $X$}
   \begin{itemize}
   \item Can write left hand side of
     (\ref{eq:one.ex2.olssolution2}) as
\begin{align*}
E_\beta&\left[ \,  \pi\{(1,1), (Y,X)\} \, \frac{\partial}{\partial \beta} \{\mu(X;\beta)\}  \{Y - \mu(X;\beta) \}\, \right]   \nonumber \\
&= E\left( \, E_\beta\left[ \,  \pi\{(1,1), (Y,X)\} \,  \frac{\partial}{\partial \beta} \{\mu(X;\beta)\} \{Y - \mu(X;\beta) \}\, | X \right] \,\right)  \\
&= E\left( \, \frac{\partial}{\partial \beta} \{\mu(X;\beta)\}  \, E_\beta\left[ \, \pi\{(1,1), (Y,X)\} \,\{Y - \mu(X;\beta) \}\, | X \right] \,\right).
\end{align*}
\item Depends \blueem{critically} on 
  $$E_\beta\left[ \, \pi\{(1,1), (Y,X)\} \,\{Y - \mu(X;\beta) \}\, | X \right],$$
\blueem{covariance} between $\pi\{(1,1),(Y,X)\}$ and $\{Y -
\mu(X;\beta) \}$ $ \Longrightarrow$ \blueem{nonzero} in general
even if $\beta = \beta_0$
\item \blueem{Thus:}  $\hatbeta^c$ solving (\ref{eq:one.ex2.ols}) is
  \blueem{almost certainly inconsistent} in general

     \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

 \redbf{Case 2 - $\pi\{(1,1), (Y, X)\}$ depends on $Y$, continued:}
\begin{itemize}
\item \blueem{Intuition:} Missingness mechanisms where $\pi\{(1,1), (Y, X)\}$ depends on $Y$
\blueem{likely MNAR}

\item \blueem{However:} It \blueem{is possible} for the mechanism
  to be \blueem{MAR}

\item \blueem{Demonstration:}  Suppose $Y$ \blueem{always
    observed}\, $X$ can be \blueem{missing} and 
probability that $X$ is missing \blueem{depends only on $Y$} $
\Longrightarrow$ 
$$\pi\{(0,1), (Y,X)\} = \pi\{(0,0),(Y,X)\} = 0 \,\,\,\mbox{ because $Y$ is always observed}$$
$$\pi\{(1,1), (Y,X)\} \,\,\, \mbox{ and } \,\,\, \pi\{(1,0), (Y,X)\} \,\,\,\mbox{  depend only on $Y$}.$$

\item \blueem{Thus:} Probabilities of \blueem{all missingness
    patterns} depend \blueem{at most} only on $Y$, which is always
  observed $ \Longrightarrow$ \blueem{MAR}
\end{itemize}

\vspace*{0.1in}

\redbf{Result:} Under MAR or MNAR, the complete case OLS estimator
is \blueem{inconsistent in general} when $\pi\{(1,1), (Y, X)\}$
depends on $Y$
 \end{frame}

 \begin{frame}
   \frametitle{Example 2: Missingness in regression analysis}

   \redbf{Case 3 - $\pi\{(1,1), (Y, X)\}$ does not depend on $(Y,X)$:}
   \begin{itemize}
   \item \blueem{Implies:} $\pi\{(1,1), (Y,X)\} = \pi\{(1,1)\}$ is a
     \blueem{constant}  $ \Longrightarrow$  true if missingness
     mechanism in \blueem{MCAR}

   \item $\pi\{(1,1)\}$ \blueem{factors out} of the left hand side
     of (\ref{eq:one.ex2.olssolution2}) $ \Longrightarrow$ with
     \blueem{correctly specified model} $\mu(x; \beta)$, (\ref{eq:one.ex2.olssolution2})
     is \blueem{trivially true}

     \item \blueem{Thus:}  Under MCAR, $\hatbeta^c$ is \blueem{consistent}
     \end{itemize}

     \vspace*{0.15in}

     \redbf{Moral:} This example illustrates how
     \blueem{complicated and sometimes nonintuitive} the
     consequences of \blueem{ignoring} the fact there are missing
     data can be
     
 \end{frame}

  \begin{frame}
    \frametitle{Example 3: Missingness in longitudinal regression}

    \redbf{Simplest situation:} Scalar outcome collected $T$ times
    $t_1,\ldots,t_T$, \blueem{no additional covariates or other
      information}\,, $ \Longrightarrow$ \blueem{full data}
    $$Z = (Y_1,\ldots,Y_T)$$
    %\vspace*{-0.2in}
\begin{itemize}
\item $Y = (Y_1,\ldots,Y_T)^T$, \blueem{interest in model for
    $E(Y)$}
    $$\mu(\beta) = \{ \mu_1(\beta),\ldots,\mu_T(\beta)
    \}^T,\hspace*{0.15in}  \beta \mbox{ is } (p \times 1)$$
\item $\mu_j(\beta)$ depends \blueem{only on time}\,; e.g.,
$$\mu_j(\beta) = \beta_0 + \beta_1 t_j \,\,\,\mbox{  or  }\,\,\, 
\mu_j(\beta) = 1/\{1 + e^{-(\beta_0 + \beta_1 t_j)}\}$$
\item $R = (R_1,\ldots,R_T)$  
  \end{itemize}
   
  \end{frame}

 \begin{frame}
   \frametitle{Example 3: Missingness in longitudinal regression}

   \redbf{Monotone missingness:}  Possible values of $R$
   $$r^{(0)} = (0,0,\ldots,0), \,\,\,r^{(1)}=(1,0,\ldots,0),\,\,\, \ldots,\,\,\,r^{(T)}=(1,1,\ldots,1)$$

   \vspace*{0.1in}

   \redbf{Full data generalized estimating equation (GEE):}
   Standard approach to estimation of $\beta$ involves solving
 \begin{equation}
  \sumiN \, \mathcal{D}^T(\beta) \mathcal{V}^{-1}(\beta, \alpha) \left( \begin{array}{c} Y_{i1} - \mu_1(\beta) \\ 
\vdots \\ Y_{iT} - \mu_T(\beta) \end{array} \right) = 0 
\label{eq:one.ex3.gee1}
\end{equation}
\vspace*{0.15in}
\begin{itemize}
\item $\mathcal{D}^T(\beta)$ $(p \times T)$ = matrix of 
  of \blueem{partial derivatives} of $\mu(\beta)$

\item \blueem{Working covariance matrix} $\mathcal{V}(\beta, \alpha)$ $(T
  \times T)$ for \blueem{correlation parameter} $\alpha$ (assume
  $\alpha$ \blueem{known} for now)

  \item \blueem{Well known:} If $\mu(\beta)$ is \blueem{correctly
      specified}\,, (\ref{eq:one.ex3.gee1}) is an \blueem{unbiased
      estimating equation} $ \Longrightarrow$ \blueem{
      consistent} estimator for  $\beta$ 
\end{itemize}
  
 \end{frame}

 \begin{frame}
   \frametitle{Example 3: Missingness in longitudinal regression}

   \redbf{Default in most software when some components of $Y$
     missing:} Estimate $\beta$ by solving (\ref{eq:one.ex3.gee1})
   based on the \blueem{available (observed) data}

   \vspace*{0.2in}

   \redbf{Implications for inference on $\beta$?}
   \begin{itemize}
\item \blueem{Monotone missingness:}  Observed data on individual
  $i$ are $R_i$ and, when  $R_i = r^{(j)}$, $(Y_{i1},\ldots,Y_{ij})^T$

\item \blueem{Result:} Contribution to \blueem{observed data} GEE depends on
  $(Y_{i1},\ldots,Y_{ij})^T$ and \blueem{corresponding submatrices}
  $\mathcal{D}^T_j(\beta)$ $(p \times j)$ and $\mathcal{V}_j(\beta,\alpha)$
  $(j \times j)$ of $\mathcal{D}^T(\beta)$ and $\mathcal{V}(\beta,\alpha)$

  \end{itemize}
\end{frame}

 \begin{frame}
   \frametitle{Example 3: Missingness in longitudinal regression}
  
\redbf{Available (observed) data GEE:}  Solve
  \begin{equation}
 \sumiN \left\{\sum^T_{j=1}  I(R_i = r^{(j)}) \,\mathcal{D}^T_j(\beta)
   \mathcal{V}^{-1}_j(\beta,\alpha) \left( \begin{array}{c} Y_{i1} - \mu_1(\beta) \\ 
\vdots \\ Y_{ij} - \mu_j(\beta) \end{array} \right) \right\} = 0
\label{eq:one.ex3.gee2}
\end{equation}
\begin{itemize}
\item \blueem{No contribution for $j=0$} as no outcomes are observed
\end{itemize}

\vspace*{0.1in}

\redbf{As in Example 2:} $\hatbeta^{obs}$ solving
(\ref{eq:one.ex3.gee2}) \blueem{converges in probability} as 
$N \rightarrow \infty$ to $\beta_0$ if \blueem{expectation of a summand = 0} at $\beta_0$
\begin{itemize}
\item \blueem{I.e., if:}  At $\beta=\beta_0$,  for $j = 1,\ldots,T$
\begin{equation}
E_\beta\left\{  I(R_i = r^{(j)}) \, \mathcal{D}^T_j(\beta)
  \mathcal{V}^{-1}_j(\beta,\alpha)
\left( \begin{array}{c} Y_{i1} - \mu_1(\beta) \\ 
\vdots \\ Y_{ij} - \mu_j(\beta) \end{array} \right) \right\} =0 
\label{eq:one.ex3.gee3}
\end{equation}


 \end{itemize} 

 \end{frame}

 \begin{frame}
   \frametitle{Example 3: Missingness in longitudinal regression}

\redbf{Recall:}  $E\{ I(R = r) | Y\} = \pr(R = r | Y) = \pi(r, Y)$
\begin{itemize}
\item Consider left hand side of (\ref{eq:one.ex3.gee3}) under
  \blueem{different conditions} on $\pi(r,Y)$
\end{itemize}

\vspace*{0.1in}

\redbf{Case 1 - $\pi(r,Y)$ does not depend on $Y$:}
\begin{itemize}
\item $\pi(r,Y) = \pi(r)$, a \blueem{constant} for $r = r^{(j)}$,
  $j=0,\ldots,T$, so missingness mechanism is \blueem{MCAR}

  \item \blueem{Can show:} Left hand side of (\ref{eq:one.ex3.gee3}) equals
$$\pi( r^{(j)} ) \mathcal{D}^T_j(\beta) \mathcal{V}^{-1}_j(\beta,\alpha) E_\beta\left\{ \left( \begin{array}{c} Y_{i1} - \mu_1(\beta) \\ 
\vdots \\ Y_{ij} - \mu_j(\beta) \end{array} \right) \right\}$$
$ \Longrightarrow$ is = 0 at $\beta=\beta_0 $ when $\mu(\beta)$ is
\blueem{correctly specified}

\item \blueem{Thus:}    $\hatbeta^{obs}$ is \blueem{consistent}
  under MCAR
  \end{itemize}
   
 \end{frame}

 \begin{frame}
   \frametitle{Example 3: Missingness in longitudinal regression}

   \redbf{Case 2 - $\pi(r^{(j)},Y)$ depends only on $Y_1,\ldots,Y_j$:}
   $$\pi(r^{(j)},Y) = \pi(r^{(j)},Y_1,\ldots,Y_j)$$
   \vspace*{-0.15in}
   \begin{itemize}
   \item $\pi(r^{(j)},Y)$ depends \blueem{only on data observed
       under $r^{(j)}$}\,, so missingness mechanism is \blueem{MAR}
\item  \blueem{Can show:} By \blueem{inner conditioning} on
  $(Y_1,\ldots,Y_j)$, left hand side of (\ref{eq:one.ex3.gee3}) equals
$$\mathcal{D}^T_j(\beta) \mathcal{V}^{-1}_j(\beta,\alpha)\,\,
 E_\beta\left\{ \pi(r^{(j)},Y_1,\ldots,Y_j)  \,\left( \begin{array}{c} Y_{i1} - \mu_1(\beta) \\ 
                                                        \vdots \\ Y_{ij} - \mu_j(\beta) \end{array} \right) \right\}$$
$ \Longrightarrow$ $k$th element in the expectation, $k = 1,\ldots,j$, equals
$$\cov\{ \pi(r^{(j)},Y_1,\ldots,Y_j) , Y_k\} \hspace*{0.15in} \neq 0
\mbox{  in general}$$

\item \blueem{Thus:} $\hatbeta^{obs}$ \blueem{is not consistent}
  in general under MAR

  \item \blueem{Clearly:} Similar under MNAR
     \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Example 3: Missingness in longitudinal regression}
   
\redbf{Result:}  The estimator for $\beta$ obtained by solving a GEE
based on the \blueem{available, observed data under monotone missingness} is
only guaranteed to be consistent \blueem{if the missingness mechanism is MCAR}
\begin{itemize}
\item MCAR is \blueem{very unlikely} to hold when the individuals
  are \blueem{humans}
  
\item \blueem{In practice:}  Fitting longitudinal models based on the
  observed data is \blueem{ubiquitous} $ \Longrightarrow$
  potential for \blueem{erroneous inference}

\item In  Chapter~\ref{s:ipw} we discuss how \blueem{inverse probability
  weighting} is one approach to ``\blueem{correcting}'' this
analysis when missingness can be assumed to be MAR
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Moral}

  \redbf{From these examples:} \blueem{Great care} must be taken
  with \blueem{default analyses using standard software for popular
    statistical methods}
  \begin{itemize}
\item \blueem{Invalid inferences} can result from \blueem{complete
    case} or \blueem{observed data} analyses when data are missing

  \item \blueem{Responsibility:} Data analyst must \blueem{be
      aware} and \blueem{think carefully and ask} about the nature of the
    missingness

  \item \blueem{Danger:} Nonstatistician users assume erroneously
    that, because the default analysis is \blueem{available in the
      software}\,, it is valid
    \end{itemize}
 
 \end{frame}

\subsubsection{Review of estimating equations}
\label{ss:reviewee}
 
\begin{frame}
  \frametitle{1.  Introduction and Motivation}
  %\label{s:intro}

 \redbf{1.5.  Review of estimating equations}

\end{frame}

\begin{frame}
   \frametitle{Recap}

   \redbf{Popular methods:} Involve estimating finite-dimensional
   \blueem{parameters of interest} in a \blueem{statistical model}
   by solving \blueem{estimating equations}
   \begin{itemize}

   \item \blueem{Regression analysis:} $Z = (Y, X)$, $E(Y|X=x)$ represented by
     $\mu(x; \beta)$, OLS estimator solves 
$$\sumiN \frac{\partial}{\partial \beta} \{\mu(X;\beta)\}  \{ Y_i -
\mu(X_i; \beta)\} = 0$$
OLS estimator is also \blueem{maximum likelihood estimator} further assuming
$Y \mid X$ is \blueem{normal}

\item \blueem{Longitudinal analysis:}  $Z=(Y_1,\ldots,Y_T)$, $E\{
  (Y_1,\ldots,Y_T)^T\}$ represented by $\mu(\beta) = \{
  \mu_1(\beta),\ldots,\mu_T(\beta) \}^T$, GEE estimator solves 
  $$\sumiN \, \mathcal{D}^T(\beta) \mathcal{V}^{-1}(\beta,\alpha) \left( \begin{array}{c} Y_{i1} - \mu_1(\beta) \\ 
\vdots \\ Y_{iT} - \mu_T(\beta) \end{array} \right) = 0$$   
\end{itemize}

\end{frame}

 \begin{frame}
   \frametitle{In general}

\redbf{M-estimation:}  The estimators solving these estimating
equations are examples in the general class of \blueem{M-estimators}
\begin{itemize}
\item \blueem{Full data inference:} Often can be addressed through
  M-estimators

  \item \blueem{Thus:}  A brief, generic review of essential elements
    of \blueem{M-estimation} in full data problems
  \end{itemize}

  \end{frame}

 \begin{frame}
   \frametitle{In general}

  \redbf{Generic statistical model:}  For the \blueem{full data} $Z$
  \begin{itemize}
  \item $Z$ has \blueem{true density} $p_Z(z)$, sample data
  $$Z_i, \,\,\, i=1,\ldots,N \,\,\,\, \mbox{ iid }$$  
\item \blueem{Goal:} Inference on $p$-dimensional $\theta$

  \item \blueem{Fully parametric model:}  $p_Z(z; \theta)$, $\theta$
    fully characterizes the density

    \item \blueem{Semiparametric model:} $\theta$ characterizes
      feature(s) of the distribution of $Z$, e.g., expectation

      \item \blueem{Assume here:}  The model is \blueem{correctly
          specified} $ \Longrightarrow$ $\theta_0$ is the
        \blueem{true value} of $\theta$ such that $p_Z(z; \theta_0)
        = p_Z(z)$ is the true density (fully parametric) or $\theta_0$ yields the
        \blueem{true feature(s)}
        \end{itemize}
  
 \end{frame}

 \begin{frame}
   \frametitle{M-estimator}

   \redbf{Definition:} An \blueem{M-estimator} for $\theta$ is the
   solution $\hattheta$ (assuming a solution exists) to the
   $(p \times 1)$ system of equations
\begin{equation}
\sumiN \, M(Z_i; \theta) = 0 
\label{eq:one.review5}
\end{equation}
\begin{itemize}
  \item $M(z; \theta) = \{ M_1(z; \theta), \ldots, M_p(z; \theta)
    \}^T$ is a $(p \times 1)$ \blueem{unbiased estimating function}
    satisfying
 $$E_\theta\{ M(Z; \theta)\} = 0 \,\,\,  \mbox{ for all } \theta$$

 \item \blueem{In addition:}   $E_\theta\{ M(Z; \theta)^T
   M(Z; \theta)\} < \infty$

\item \blueem{And:} $E_\theta\{ M(Z; \theta) M(Z; \theta)
^T\}$ $(p \times p)$ is positive definite for all $\theta$
\end{itemize}
   
   \end{frame}
   
\begin{frame}
   \frametitle{M-estimator}

   \redbf{For example:}
   \begin{itemize} 
   \item \blueem{OLS estimator:}
     $$M(Z; \beta) = \frac{\partial}{\partial \beta} \{\mu(X;\beta)\}
     \{ Y - \mu(X; \beta)\}$$

   \item \blueem{Maximum likelihood estimator:} Fully parametric model  $p_Z(z; \theta)$
     $$M(z ;\theta) = \frac{\partial}{\partial \theta} \log\{ p_Z(z; \theta)\}$$
i.e., the \blueem{score}\,, and the estimating equation is the
\blueem{score equation}
     \end{itemize}
   
 \end{frame}

 \begin{frame}
   \frametitle{M-estimator result}

\redbf{Under regularity conditions:}  $\hattheta$ satisfying
(\ref{eq:one.review5})  is a \blueem{consistent and asymptotically
  normal estimator} for $\theta$
$$\hattheta \inp \theta_0$$
\begin{equation}
  N^{1/2}( \hattheta - \theta_0) \inL \N(0, \Sigma)
\label{eq:one.inL}
  \end{equation}
\begin{itemize}
  \item $\inL$ denotes \blueem{convergence in distribution (law)}
\item (\ref{eq:one.inL}) is \blueem{shorthand} meaning the left hand
  side converges in distributions to a random vector with $\N(0,
  \Sigma)$ distribution (covariance matrix $\Sigma$)
   \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Standard M-estimator argument}

\redbf{First-order Taylor series in each row of
  (\ref{eq:one.review5}):}
$$0 = N^{-1/2} \sumiN \, M(Z_i; \hattheta) = N^{-1/2} \sumiN \, M(Z_i; \theta_0) 
+ \left\{ N^{-1} \sumiN \frac{\partial}{\partial \theta^T} M(Z_i; \theta^*)\right\} N^{1/2} (\hattheta-\theta_0)$$
\begin{displaymath}
\frac{\partial}{\partial \theta^T} M(z; \theta^*) = 
\left( 
\begin{array}{ccc}
\ds{\left.\frac{\partial \, M_{1}(z;\theta)}{\partial \theta_1}\right|_{\theta=\theta^{(1)*}}}, & \dots, & 
\ds{\left.\frac{\partial \,M_{1}(z;\theta)}{\partial \theta_p]}\right|_{\theta=\theta^{(1)*}}} \\
\vdots & \ddots &  \vdots\\
\ds{\left.\frac{\partial \,M_{p}(z;\theta)}{\partial \theta_1}\right|_{\theta=\theta^{(p)*}}}, & \dots, & 
\ds{\left.\frac{\partial \,M_{p}(z;\theta)}{\partial \theta_p}\right|_{\theta=\theta^{(p)*}}}
\end{array} \right)
\end{displaymath}
$\theta^{(j)*}$ is a value between $\hattheta$ and $\theta_0$, $j=1,\ldots,p$
 \end{frame}

 \begin{frame}
   \frametitle{Standard M-estimator argument}

\redbf{First-order Taylor series in each row of
  (\ref{eq:one.review5}), continued:}
\begin{itemize}
\item By \blueem{consistency} of  $\hattheta$ and because
  $\theta^{(j)*}$, $j=1,\ldots,p$, is between $\hattheta$ and
  $\theta_0$, so that $\theta^{(j)*} \inp \theta_0$
$$\left\{ N^{-1} \sumiN \frac{\partial}{\partial \theta^T} M(Z_i; \theta^*)\right\} \inp
E\left\{ \frac{\partial}{\partial \theta^T} M(Z; \theta_0)\right\}$$
\item Assuming this matrix is \blueem{nonsingular}\,, with
  \blueem{increasing probability} as $N \rightarrow \infty$, so is
  the left hand side, and
  $$\left\{ N^{-1} \sumiN \frac{\partial}{\partial \theta^T} M(Z_i; \theta^*)\right\}^{-1} \inp
\left[E\left\{ \frac{\partial}{\partial \theta^T} M(Z; \theta_0)\right\}\right]^{-1}$$

  \end{itemize}
   
 \end{frame}

 \begin{frame}
   \frametitle{Standard M-estimator argument}
   \phantomsection\label{slide:standMest}

\redbf{Rearranging and using these results:}
$$N^{1/2} (\hattheta-\theta_0) = 
- \left[ E\left\{ \frac{\partial}{\partial \theta^T} M(Z; \theta^*)\right\}\right]^{-1} 
N^{-1/2} \sumiN \, M(Z_i; \theta_0)$$
\begin{itemize}
\item  By \blueem{central limit theorem}
$$N^{-1/2} \sumiN \, M(Z_i; \theta_0) \inL \N\big[ 0, E\{ M(Z;
\theta_0) M(Z; \theta_0)^T \} \big]$$
\item By \blueem{Slutsky's theroem}\,, and with $A^{-T} =
  (A^{-1})^T$, as in  (\ref{eq:one.inL})
  \end{itemize}
  $$N^{1/2} (\hattheta-\theta_0) \inL  \N( 0, \Sigma)$$
 $$\hspace*{-0.15in}\Sigma = \left[ E\left\{ \frac{\partial}{\partial \theta^T} M(Z; \theta_0)\right\}\right]^{-1} \!\!
E\{ M(Z; \theta_0) M(Z; \theta_0)^T \} \left[ E\left\{ \frac{\partial}{\partial \theta^T} M(Z; \theta_0)\right\}\right]^{-T}$$

 \end{frame}

 \begin{frame}
   \frametitle{Approximate sampling distribution for $\hattheta$}

\redbf{Estimator for $\Sigma$:}  By WLLN
  $$N^{-1} \sumiN \frac{\partial}{\partial \theta^T} M(Z_i; \theta_0) \inp 
E\left\{ \frac{\partial}{\partial \theta^T} M(Z; \theta_0)\right\}$$
$$N^{-1} \sumiN M(Z_i; \theta_0) M(_iZ; \theta_0)^T \inp E\{ M(Z;
\theta_0) M(Z; \theta_0)^T \}.$$
$ \Longrightarrow$ \blueem{Substitute} $\hattheta$ for $\theta_0$
\begin{align*}
\hat{\Sigma} = 
\left\{ N^{-1} \sumiN \frac{\partial}{\partial \theta^T} M(Z_i; \hattheta)\right\}^{-1} \!\!
&\left\{ N^{-1} \sumiN M(Z_i; \hattheta) M(_iZ; \hattheta)^T \right\} \\
&\times \left\{ N^{-1} \sumiN \frac{\partial}{\partial \theta^T} M(Z_i;   \hattheta)\right\}^{-T}
                                          \end{align*}
\begin{itemize}

\item \blueem{Sandwich estimator}

\end{itemize}


\end{frame}

 \begin{frame}
   \frametitle{Approximate sampling distribution for $\hattheta$}

\redbf{Combining these results:}  $\hattheta$ has approximate
sampling distribution
$$\hattheta  \simdot \N ( \theta_0, N^{-1} \hat{\Sigma})$$
\vspace*{-0.1in}
\begin{align*} 
N^{-1} \hat{\Sigma} = 
\left\{ \sumiN \frac{\partial}{\partial \theta^T} M(Z_i; \hattheta)\right\}^{-1} \!\!
 &\left\{ \sumiN M(Z_i; \hattheta) M(_iZ; \hattheta)^T \right\}\\
&\times \left\{ \sumiN \frac{\partial}{\partial \theta^T} M(Z_i;
  \hattheta)\right\}^{-T}
  \end{align*}
\begin{itemize}
\item ``$\simdot$'' denotes ``approximately distributed as'' 

\item Rescaling results in the $N^{-1}$ terms \blueem{canceling out}

  \item  Can use to drive \blueem{approximate (large sample)}
    standard errors and confidence intervals for components of
    $\hattheta$ and $\theta_0$
  \end{itemize}

\end{frame}

 \begin{frame}
   \frametitle{Approximate sampling distribution for $\hattheta$}

\redbf{Often:}  Only a \blueem{subset} of the
components of $\theta$ $(p \times 1)$ is of interest
\begin{itemize}
\item The remaining components are \blueem{nuisance parameters}

  \item E.g., \blueem{longitudinal regression} with
    \blueem{mean model} for $E(Y)$ $\mu(\beta)$ and \blueem{
      working covariance matrix} $\mathcal{V}(\beta,\alpha)$ with
    $\alpha$ \blueem{unknown} $ \Longrightarrow$ $\theta =
    (\beta^T,\alpha^T)^T$ for $\beta$ $(r \times 1)$, $\alpha$ $(s
    \times 1)$, $p = r+s$

    \item $\alpha$ is a \blueem{nuisance parameter} but must be
      \blueem{estimated} along with $\beta$ by solving an \blueem{
        additional estimating equation} jointly with
      (\ref{eq:one.ex3.gee1}) 

      \item \blueem{Thus:} One must solve a set of \blueem{
          stacked estimating equations}
        
\begin{equation}
\sumiN M(Z_i; \theta) =   \sumiN \left( \begin{array}{c} M_1(Z_i ; \theta) \\*[0.05in]
                   M_2(Z_i ; \theta) \end{array} \right) = 0  
\label{eq:stacked}
\end{equation}   
for $M_1(z; \theta)$ $(r \times 1)$, $M_2(z; \theta)$ $(s \times 1)$
\item \blueem{Longitudinal regression:}  $M_1(z; \theta)$ is (\ref{eq:one.ex3.gee1}) 
\end{itemize}


\end{frame}

 \begin{frame}
   \frametitle{Approximate sampling distribution for $\hattheta$}

   \redbf{Stacked estimating equations:}  Solving (\ref{eq:stacked})
   in $\theta$ yields $\hattheta = (\hatbeta^T, \hatalpha^T)^T$
\begin{itemize}
\item Assuming the mean model is \blueem{correctly specified} (true
  value $\beta_0$), from the \blueem{standard M-estimator argument}
  on Slide~\pageref{slide:standMest}
  $$N^{1/2}(\hatbeta - \beta_0) \inL \N(0, \Sigma_{11})$$
$\Sigma_{11}$ is the upper left $(r \times r)$ block of $\Sigma$
$$ \Longrightarrow \,\,\,\, \hatbeta \simdot \N(\beta_0, N^{-1}\hat{\Sigma}_{11})$$
\item Form of $\Sigma_{11}$ and $\hat{\Sigma}_{11}$ can be
  obtained using standard formul{\ae} for the \blueem{inverse of a
    partitioned matrix}

\end{itemize}
 \end{frame}

 \subsubsection{Objectives and scope}
 
 \begin{frame}
  \frametitle{1.  Introduction and Motivation}
  %\label{s:intro}

 \redbf{1.6.  Objectives and scope}

\end{frame}

\begin{frame}
   \frametitle{Objective of this course}

\redbf{Despite the ubiquity of missing data in applications and the
  extensive research on methods for handling them:}
\begin{itemize}
\item Many data analysts are \blueem{uncertain} what approach to
  take when missing data arise in their work

  \item And \blueem{remarkably} many statistics/biostatistics curricula \blueem{do not
      offer} a full-fledged course on methods for analysis in the
    presence of missing data

\item \blueem{This course:}  Address this gap in practice and training

  \end{itemize}
\end{frame}

  \begin{frame}
   \frametitle{Objective of this course}

  \redbf{Goals:}
\begin{itemize}
\item  Cover \blueem{key concepts} and \blueem{
    fundamentals} underlying missing data methods in detail

\item Discuss \blueem{popular classes of methods} and their
  \blueem{implementation} in practice, including \blueem{software}
  (R, SAS)

  \item Provide the \blueem{foundation and background}  for further
    study of the literature
   \end{itemize}
 \end{frame}

 \begin{frame}
   \frametitle{Scope}

\redbf{Humans:}  As we have discussed, missing data are a
particular problem when the individuals are \blueem{humans}
\begin{itemize}
\item  Missing data and dropout are \blueem{routine} in \blueem{
    clinical trials}

  \item And there are \blueem{entire books} devoted to missing data
    in the analysis of clinical trials

    \item \blueem{Thus:}  Many of examples discussed involve
      clinical trials

      
      \item \blueem{However:}  The approaches and insights are
        \blueem{broadly relevant} to other application areas


  \end{itemize}

   
 \end{frame}

 %%%%%%%%%%   CHAPTER 2

\subsection{Na\"{i}ve Methods}
\label{s:naive}


\subsubsection{Introduction}
  
\begin{frame}
\frametitle{2.  Na\"{i}ve Methods}
%\label{s:naive}

\redbf{2.1.  Introduction}
\end{frame}
  
  \begin{frame}
    \frametitle{In this chapter}

    \redbf{Ad hoc methods:}
    \begin{itemize}
\item \blueem{Formal methods} based on a statistical framework exist
  for achieving valid inferences in the presence of missing data

  \item \blueem{However:}  In practice, \blueem{ad hoc}
    methods, which are often chosen because they are simple to
    implement, are often used

  \item ``\blueem{Na\"{i}ve methods:}\,'' \blueem{Not based} on a
    principled framework for addressing missing data

  \item \blueem{Goal:} Demonstrate the \blueem{pitfalls} of these methods

     \end{itemize} 

    \end{frame}

\subsubsection{Complete or available case analysis}
    
\begin{frame}
\frametitle{2.  Na\"{i}ve Methods}

\redbf{2.2.  Complete or available case analysis}

      \end{frame}

\begin{frame}
  \frametitle{Definitions}

  \redbf{Complete cases:}  Data from individuals on whom the
  \blueem{full data} (all data intended to be collected) are \blueem{
    observed}
  \begin{itemize}
    \item E.g., in \blueem{regression}\,, data from all individuals
      who have the outcome $Y$ and \blueem{all components} of
      covariate vector $X$ \blueem{observed}
    \end{itemize}

    \vspace*{0.2in}

    \redbf{Available cases:} \blueem{Observed} data from all
    individuals, including \blueem{partial information}
    \begin{itemize}
      \item E.g., with \blueem{longitudinal outcome} and \blueem{
          dropout}\,, observed outcomes for all individuals \blueem{up to the
        time of dropout}
    \end{itemize}

    \vspace*{0.2in}

\redbf{Note:}  This terminology may not be used \blueem{consistently} in the literature

\end{frame}

\begin{frame}
  \frametitle{History}

\redbf{Before modern computing:}  Great importance for \blueem{computation} of having a
\blueem{rectangular data set}
\begin{itemize}
\item I.e., if $M$  \blueem{variables} were to be collected on
each of $N$ \blueem{individuals}\,, the intended data are
represented as a \blueem{$(N \times M)$ rectangular array}

\item $N$ \blueem{rows}\,, with each row (\blueem{data record}\,) containing
    $M$ values \\ $ \Longrightarrow$  $M$ \blueem{columns}

  \item \blueem{Implementation} of common analysis
    methods \blueem{depended on this rectangular structure} because
    computations were often \blueem{simpler and less time consuming}
    than without this structure

  \item \blueem{Moreover:} Straightforward to deduce \blueem{
      finite-sample properties} of methods (not easy without
    rectangular structure)
  \end{itemize}

  \vspace*{0.1in}
  
\redbf{Result:}  Analysis based \blueem{just on the complete
    cases} was viewed primarily as a way to \blueem{preserve
    rectangular structure} and \blueem{simplify computations and
    properties}
  

\end{frame}


\begin{frame}
  \frametitle{History}

  \redbf{Downsides:}  At the time
  \begin{itemize}
\item  Possibility for \blueem{biased inferences:}  Not \blueem{really
  appreciated}  

  \item \blueem{Primary concern:}  Extent of \blueem{loss of
      information}

  \item \blueem{For example:}  From Molenberghs and Kenward (2007), with $M = 20$ variables,
10\% missingness of each, and (somewhat unrealistically) missingness
in each variable happening \blueem{independently} of that in all 
others $ \Longrightarrow$ \blueem{probability of observing a
  complete case $\approx 0.13$}

\item \blueem{Thus:}  Even with MCAR, \blueem{substantial} loss of
  precision and power

  \end{itemize} 

\end{frame}


\begin{frame}
  \frametitle{History}

\redbf{Longitudinal data:}  With the advent of \blueem{GEEs} in
the mid-1980s, which \blueem{do not require} a rectangular array of
data and the availability of  \blueem{software} for solving GEEs
\begin{itemize}
\item Analysts often carried out analyses based on the \blueem{
    available cases} because they \blueem{could}

  \item Without consideration of the \blueem{possible dire}
    consequences of missingness for valid inference

    \item \blueem{Danger:} As in Chapter~\ref{s:intro}, where we
      saw that inference on parameters characterizing
        population mean outcome over time can be \blueem{
          compromised} even if the missingness mechanism is \blueem{MAR}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Complete and available case analysis}

  \redbf{Result:}
  \begin{itemize}

  \item \blueem{Moral:} Even though an analysis is \blueem{
      feasible} to conduct, it must be interpreted through the lens of
    \blueem{implications of missing data}

\item \blueem{Thus:}  Although \blueem{complete case} and
  \blueem{available case}
approaches are \blueem{straightforward} to implement, they are
\blueem{not recommended}

\item And we do not discuss them further

\end{itemize}
\end{frame}

\subsubsection{Simple imputation methods}

\begin{frame}
\frametitle{2.  Na\"{i}ve Methods}

\redbf{2.3.  Simple imputation methods}
\end{frame}


\begin{frame}
  \frametitle{Imputation}

  \redbf{Intuitive approach:}  ``\blueem{Fill in}\,'' the missing
  values (somehow)
  \begin{itemize}

  \item I.e., \blueem{impute} the missing values

  \item And carry out the intended analysis that \blueem{would be
      undertaken} if the full data were observed
    \end{itemize}

    \vspace*{0.2in}

    \redbf{Challenge:} How to impute?
    \begin{itemize}
    \item \blueem{Historically:}  \blueem{Simple, ad hoc} approaches

      \item \blueem{Problem:}  Many simple approaches can be shown to
        lead to \blueem{valid inferences} only when the missingness
        mechanism is \blueem{MCAR} (coming up)

    \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Imputation}

  \redbf{Challenge:}  Valid \blueem{measures of uncertainty}
  \begin{itemize}
  \item \blueem{Temptation:} Proceed as if the imputed observations
    are the \blueem{intended observations}

    \item And obtain standard errors, confidence intervals, etc via
      \blueem{usual formul{\ae}}

      \item \blueem{Critical:} This practice \blueem{does not
          acknowledge} that imputed data are \blueem{derived} from
        the observed data

        \item And thus fails to account for \blueem{uncertainty due to the imputation of missing
            values}
        \end{itemize}

        \vspace*{0.1in}

        \redbf{Here:}  Discuss some \blueem{simple}
        imputation methods with these drawbacks
        \begin{itemize}

          \item In Chapter~\ref{s:mi}, we discuss \blueem{principled approaches}
            based on imputation
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Unconditional mean imputation}

\redbf{Natural but simplistic:}  Impute missing value of a \blueem{
  continuous variable} using the \blueem{average of observed values}
(from all individuals for whom the variable is observed)
\begin{itemize}

\item ``\blueem{Unconditional mean imputation}\,'' because does not
  use (\blueem{condition on}\,) other information on an individual
  for whom the variable is missing

\item \blueem{Discrete variables:}  Use the \blueem{mode} of
  observed values
\end{itemize}

\vspace*{0.2in}

\redbf{Not recommended:}  Although \blueem{straightforward} to
implement, intuitively, inference seems \blueem{suspect}
    
  \end{frame}

\begin{frame}
    \frametitle{Regression imputation}

    \redbf{More promising:}  Posit a \blueem{regression model}
    for a variable with missingness as a function of \blueem{
      observed variables} and use \blueem{predicted values} from the
    fit to \blueem{impute} missing values

\vspace*{0.1in}

\redbf{Example:}  \blueem{Full data} $Z = (Y_1,Y_2)$, $K=2$
\begin{itemize}
\item $Y_1$ is the \blueem{baseline} measurement of some outcome

\item $Y_2$ is the \blueem{follow up} measurement of the outcome

  \item $Y_1$ is \blueem{always observed} ($R_1=1$ always), $Y_2$
    can be \blueem{missing} ($R_2 = 0$ or 1)

  \item \blueem{Define:} $Y = (Y_1,Y_2)^T$, $\tilY_1 = (1, Y_1)^T$

    \item \blueem{Objective:}  Estimate $\mu_2 = E(Y_2)$
  \end{itemize}
   
  \end{frame}


  \begin{frame}
    \frametitle{Regression imputation}

    \redbf{Example - imputation model:} To \blueem{impute} missing
    $Y_2$, posit a \blueem{linear
      regression model}
    \begin{equation}
E(Y_2 | Y_1) = \beta_0 + \beta_1 Y_1 = \tilY_1^T \beta, \hspace{0.15in} \beta 
= (\beta_0,\beta_1)^T
\label{eq:two.2}
\end{equation}
\begin{itemize}
\item Given a random sample of $N$ individuals with \blueem{observed
    data}, \blueem{fit} (\ref{eq:two.2}) using the \blueem{
    complete cases} for whom $R_2=1$

\item \blueem{OLS estimator based on complete cases:}
  \begin{equation}
\hatbeta = \left\{ \sumiN R_{i2} \tilY_{i1} \tilY_{i1}^T \right\}^{-1} \, 
\left\{ \sumiN R_{i2} \tilY_{i1} Y_{i2} \right\}
\label{eq:two.3}
\end{equation}

\item \blueem{Impute $Y_2$} for individuals for whom it is \blueem{
    missing} ($R_2=0$) by \blueem{predicted values}
  $$\hatY_{i2} = \tilY_{i1}^T \hatbeta = \hatbeta_0 + \hatbeta_1 Y_{i1}$$
 \end{itemize} 

  \end{frame}

  \begin{frame}
    \frametitle{Regression imputation}

    \redbf{Example - regression imputation estimator:}
    \begin{itemize}
    \item \blueem{Imputed data set:} $(Y_{i1},Y_{i2})$ for complete
    cases $i$ with $(R_{i1},R_{i2}) = (1,1)$; $(Y_{i1}, \hatY_{i2})$
    for $i$ with $Y_2$ missing $(R_{i1},R_{i2}) = (1,0)$

\item \blueem{Estimator for $\mu_2$:}  Based on the imputed data set
$$\hatmu^{RIMP}_2 = N^{-1} \sumiN \Big\{ R_{i2} Y_{i2} + (1 - R_{i2}) \hatY_{i2} \Big\}$$
\end{itemize}

\vspace*{0.2in}

\redbf{When is $\hatmu^{RIMP}_2$ a consistent estimator for the true
  value of $\mu_2$?}

  \end{frame}

  \begin{frame}
    \frametitle{Regression imputation}
    \phantomsection\label{slide:regimpconditions}

    \redbf{Example - regression imputation estimator:} Consider
    properties of $\hatmu^{RIMP}_2$ under \blueem{one or both} of the following
    conditions
    \begin{itemize}
\item[ (i)] \blueem{In truth:}  $E(Y_2 | Y_1) = \beta^{(0)}_0 + \beta^{(0)}_1 Y_1 =
  \tilY_1^T \beta^{(0)}$ for some $\beta^{(0)} =
  (\beta^{(0)}_0,\beta^{(0)}_1)^T$; that is, the linear regression model
  (\ref{eq:two.2}) used for imputation is \blueem{correctly specified}\\*[0.1in]

\item[(ii)] $\pr(R_2 = 1 | Y) = \pr(R_2 = 1 | Y_1) = \pi_2(Y_1)$;
  i.e., the \blueem{missingness mechanism is MAR}
\end{itemize}

  \end{frame}

  \begin{frame}
    \frametitle{Regression imputation}

    \redbf{Example - case 1:}  \blueem{Both} (i) and (ii) on
    Slide~\pageref{slide:regimpconditions} hold
\begin{itemize}
\item By WLLN and under regularity conditions
\begin{equation}
\hatbeta \inp \left\{ E(R_2 \tilY_1 \tilY_1^T ) \right\}^{-1} \, E(R_2 \tilY_1 Y_2)
\label{eq:two.5}
\end{equation}

\item Second component of rightmost term in (\ref{eq:two.5})
  $$\{ E(R_2 Y_2),  E(R_2 Y_1 Y_2)\}^T$$
  \vspace*{-0.15in}
\begin{align*}
E(R_2 Y_1 Y_2) &=  E\{ E(R_2| Y_1,Y_2) Y_1 Y_2\} = E\{ E(R_2| Y_1) Y_1 Y_2\}\\ 
&= E\{ \pi_2( Y_1) Y_1 Y_2\} \hspace*{0.1in} \mbox{ by (ii)} \\
&=  E\{\pi_2(Y_1) Y_1 E(Y_2 | Y_1) \} = E\{ \pi_2(Y_1) Y_1 \tilY_1^T\} \beta^{(0)}
\end{align*}
\item \blueem{Similar argument} for first component
    \end{itemize}

\end{frame}

  \begin{frame}
    \frametitle{Regression imputation}

    \redbf{Example - case 1:}  \blueem{Both} (i) and (ii) on
    Slide~\pageref{slide:regimpconditions} hold
    \begin{itemize}
    \item \blueem{Thus:}  Under (i)
      $$E(R_2 \tilY_1 Y_2) = E\{ \pi_2(Y_1) \tilY_1 \tilY_1^T\} \beta^{(0)}$$

\item \blueem{Similar argument:}  Left term in (\ref{eq:two.5}) satisfies
  $$E(R_2 \tilY_1 \tilY_1^T ) = E\{ \pi_2(Y_1) \tilY_1 \tilY_1^T\}$$

\item $ \Longrightarrow$ putting together
$$\hatbeta \inp \left[ E\{ \pi_2(Y_1) \tilY_1 \tilY_1^T\}\right]^{-1} 
E\{ \pi_2(Y_1) \tilY_1 \tilY_1^T\} \beta^{(0)} = \beta^{(0)}$$
 
      \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Regression imputation}

    \redbf{Example - case 1:}  \blueem{Result:}  Under (i) and (ii) on
    Slide~\pageref{slide:regimpconditions}
\begin{align*}
\hatmu^{RIMP}_2 &= N^{-1} \sumiN \Big\{ R_{i2} Y_{i2} + (1 - R_{i2}) \tilY_{i1}^T \hatbeta\Big\} \\
                &\inp  E(R_2 Y_2) + E\{ (1-R_2) \tilY_1^T\} \beta^{(0)} \\
  &= E\{ \pi_2(Y_1) Y_2\} + E\Big[ \{ 1 - \pi_2(Y_1)\} \tilY_1^T \beta^{(0)} \Big]  \\
&=  E\{ \pi_2(Y_1) Y_2\} + E[ \{ 1 - \pi_2(Y_1)\} E(Y_2 | Y_1) ] \\
&=  E\{ \pi_2(Y_1) E(Y_2|Y_1)\} + E[ \{ 1 - \pi_2(Y_1)\} E(Y_2 | Y_1) ] \\
&=  E\{ E(Y_2|Y_1)\} = E(Y_2) = \mu_2
\end{align*}
\vspace*{-0.15in}
\begin{itemize}
\item With a \blueem{correctly specified imputation model} and
  \blueem{MAR missingness}\, $\hatmu^{RIMP}_2$ is a \blueem{
    consistent estimator} for $\mu_2$

\end{itemize}

\end{frame}

  \begin{frame}
    \frametitle{Regression imputation}

    \redbf{Example - case 2:}  (ii) on
    Slide~\pageref{slide:regimpconditions} holds (\blueem{MAR}\,) but
    (i) (\blueem{correctly specified imputation model}\,) does not
    \begin{itemize}
    \item \blueem{Here:}
      $$\hatbeta \inp \beta^* = \left[ E\{ \pi_2(Y_1) \tilY_1 \tilY_1^T\}\right]^{-1} 
E\{ \pi_2(Y_1) \tilY_1 E(Y_2 | Y_1)\}$$ 
\item And thus
  \begin{align*} 
\hatmu^{RIMP}_2 &=  N^{-1} \sumiN \Big\{ R_{i2} Y_{i2} + (1 - R_{i2}) \tilY_{i1}^T \hatbeta\Big\} \\
&\inp E\{ \pi_2(Y_1) Y_2\} + E[ \{ 1 - \pi_2(Y_1)\} \tilY_1^T ] \beta^* \\
&=  E(Y_2) + E[ \{ 1 - \pi_2(Y_1)\} (\tilY_1^T\beta^* -Y_2) ] \\
&= \mu_2 +  E[ \{ 1 - \pi_2(Y_1)\} (\tilY_1^T\beta^* -Y_2) ] 
\end{align*}
\item Evidently \blueem{not equal to} $\mu_2$ in general
      \end{itemize}
  \end{frame}

\begin{frame}
    \frametitle{Regression imputation}

    \redbf{Example - case 3:} (i) (\blueem{correctly specified
      imputation model}\,) on Slide~\pageref{slide:regimpconditions} holds
    but (ii) (\blueem{MAR}\,) does not
    \begin{itemize}
\item \blueem{Now have} 
$$\pr(R_2 = 1 | Y) = \pi_2(Y)$$
depending on \blueem{both} $Y_1$ and $Y_2$ 

\item But $Y_2$ may be \blueem{unobserved}

  \item \blueem{Result:}  Even if the regression model
    (\ref{eq:two.2}) is \blueem{correctly
specified}, the \blueem{complete case OLS estimator} $\hatbeta$ in
(\ref{eq:two.3}) is \blueem{not consistent in general} (try it)

\item And it can be shown (try it) that $\hatmu^{RIMP}_2$ \blueem{
    does not converge in probability} to $\mu_2$ in general (try it)
      \end{itemize}
    
  \end{frame}


\begin{frame}
    \frametitle{Regression imputation}

    \redbf{Result:}  In this \blueem{special case}\,, regression imputation 
yields \blueem{consistent inference} if \blueem{both} the imputation model is
\blueem{correctly specified} and the missingness mechanism is
\blueem{MAR}
\begin{itemize}
\item In more \blueem{complex settings}\,,  such a result \blueem{
    need not hold}

\item Even if (i) and (ii) \blueem{do hold}\,, the usual \blueem{
    standard error} for a sample mean \blueem{is not a valid
    estimator} for the precision of the $\hatmu^{RIMP}_2$

\item \blueem{Thus:} Using this formula as if the imputed values
  were actual observations on the outcome  leads to \blueem{incorrect assessment
    of uncertainty}

\item \blueem{For fun:}  Try deriving a valid standard error   estimator
  \end{itemize}
    
  \end{frame}

   
  \begin{frame}
    \frametitle{Other imputation methods}

\redbf{Hot deck imputation:}  Popular in analysis of \blueem{
  survey data}
\begin{itemize}
\item Use observed values from ``\blueem{matching}'' individuals to
  impute values for those individuals for whom they are missing based
  on some ``\blueem{matching}'' strategy

\item \blueem{Matching} may be based on common characteristics
  (e.g., occupation, education, income, geographic location, etc)

\item Or on a \blueem{distance measure} applied to a set of
  characteristics

  \item \blueem{Feature:} Uses \blueem{actual observations} (from
    other individuals) rather than estimated/predicted values
  \end{itemize}
    
\end{frame}

   
  \begin{frame}
    \frametitle{Other imputation methods}

  \redbf{In general:}  Most simple imputation methods require the
  missingness mechanism to be \blueem{MCAR} for \blueem{consistent
    inference}
  \begin{itemize}

  \item \blueem{Moreover:} Usual formul{\ae} for \blueem{
      estimators of precision}\,, although commonly used in practice,
    are \blueem{incorrect} because of failure to take into account
    \blueem{uncertainty due to imputation}

  \item \blueem{Multiple imputation:}  A principled framework in
    which imputation can be \blueem{justified formally} and 
\blueem{estimation of precision} takes account of imputation; coming
in Chapter~\ref{s:mi}.  

    \end{itemize}
  \end{frame}

\subsubsection{Last observation carried forward}
  
\begin{frame}
\frametitle{2.  Na\"{i}ve Methods}

\redbf{2.4.  Last observation carried forward}
\end{frame}

\begin{frame}
    \frametitle{Form of simple imputation}

    \redbf{Last observation carried forward (LOCF):}  Also referred
    to as \blueem{last value carried forward (LVCF)} 
    \begin{itemize}
\item \blueem{Commonly used:}  A variable is ascertained \blueem{
    over time } and can be \blueem{missing} in either a \blueem{
    monotone} or \blueem{nonmonotone} fashion

  \item Particularly popular when missingness is due to \blueem{dropout}

\item Discussed here for dropout in a \blueem{longitudinal study}
      \end{itemize}

\vspace*{0.1in}

\redbf{Implementation:} Data are \blueem{intended} to be collected
at $T$ time points
\begin{itemize}
\item \blueem{Dropout at time $t_j$:}  Replace missing values at times $t_j,\ldots,t_T$
by the individual's \blueem{last observed value}\,; i.e., the value
at $t_{j-1}$

\item A form of \blueem{imputation}

  \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Illustration of LOCF}

    \vspace*{-0.25in}

    \begin{center}
      \includegraphics[width=3.1in,alt={Longitudinal outcomes for
        several subjects, where the last observation on each is
        carried forward to the future time points}]{locf.png}
      \end{center}

      \end{frame}

  \begin{frame}
    \frametitle{LOCF in clinical trials}

\redbf{Discussed separately from other imputation methods:}  
\begin{itemize}
\item Has been \blueem{commonly used} in the analysis of \blueem{
    clinical trials} in which the \blueem{primary
    outcome} is collected \blueem{longitudinally}

\item Has engendered \blueem{controversy and debate}\,, particularly in pharmaceutical research and the
  regulatory context

\item See Molenberghs and Kenward (2007) and O'Kelly and Rattich
  (2014) for discussions of the main issues 
\end{itemize}

\vspace*{0.1in}

\redbf{Attempts at scientific justification of LOCF:}
    \begin{itemize}

    \item \blueem{In some settings:}  Inference of interest is on
      the \blueem{last observed outcome measure}

      \item Whether such inference addresses a \blueem{meaningful
          scientific question} is debatable and context-specific
        \end{itemize}
       \end{frame}

  \begin{frame}
    \frametitle{LOCF in clinical trials}

  \redbf{Attempts at scientific justification of LOCF, continued:}
    \begin{itemize}
  
\item \blueem{Common contention:}  LOCF represents a
  ``\blueem{conservative analysis}\,'' in clinical trials comparing
  experimental treatment to control, where outcome under treatment is
  expected to \blueem{improve over time}

\item Replacing missing values from dropout forward makes
  the longitudinal profile for an individual assigned to treatment
  look \blueem{worse} than it presumably would have if he/she had
  continued on treatment

\item Thus ``\blueem{handicapping}\,'' the experimental treatment in
  the comparison with control, so if treatment \blueem{still shows a 
    statistically significant difference from the control} based on
  comparison at final time $T$, supports \blueem{superiority} of treatment

\item \blueem{However:}  This idea has \blueem{no rigorous
    basis}\,, and examples can be constructed where this conservatism
  \blueem{does not hold}  (see Molenberghs and Kenward, 2007, Chapter 4)

      \end{itemize}

    
  \end{frame}


  \begin{frame}
    \frametitle{LOCF in a longitudinal study}

\redbf{Example:}  Observations on a scalar outcome are to be taken
at $t_1,\ldots,t_T$, yielding intended \blueem{full data} $Z =
(Y_1,\ldots,Y_T)$
\begin{itemize}
\item As usual, $R = (R_1,\ldots,R_T)$
\item \blueem{Dropout:}  All individuals observed at \blueem{
    baseline} ($R_1=1$ for all) but can drop out thereafter

  \item \blueem{Interest:}  Inference on 
    $$\mu_T = E(Y_T)$$

  \item \blueem{LOCF estimator for $\mu_T$:} \blueem{Sample mean},
    where $Y_T$ is used when observed, last observed value is
    used in place of $Y_T$ if not

  \item Using \blueem{dropout notation:}  The LOCF estimator is
$$\hatmu^{LOCF}_T = N^{-1} \sumiN \sum^T_{j=1} I(D_i = j+1) Y_{ij}$$
$ \Longrightarrow$ if dropout at time $j+1$, $j<T$, $Y_j$ is used
in place of $Y_T$; else, if  $j=T$, observed $Y_T$ is used
    
  \end{itemize}

    
  \end{frame}

  \begin{frame}
    \frametitle{LOCF in a longitudinal study}

\redbf{Example, continued:}  \blueem{Large sample behavior} of $\hatmu^{LOCF}_T$?
\begin{align}
\hatmu^{LOCF}_T &=  N^{-1} \sumiN \sum^T_{j=1} I(D_i = j+1) Y_{ij}  \nonumber \\
&=  N^{-1} \sumiN \Big\{ Y_{iT} + \sum^T_{j=1}  I(D_i = j+1) Y_{ij}
- \sum^T_{j=1}  I(D_i = j+1) Y_{iT} \Big\} \nonumber \\ 
&=  N^{-1} \sumiN \Big\{ Y_{iT} - \sum^{T-1}_{j=1} I(D_i = j+1)  (Y_{iT}-Y_{ij})\Big\} \nonumber \\
&\inp E(Y_T) - \sum^{T-1}_{j=1} E\{ I(D = j+1)(Y_T-Y_j) \} \nonumber \\
&=  E(Y_T) - \sum^{T-1}_{j=1} E\{ \pr(D=j+1|Z) (Y_T-Y_j) \}
\label{eq:two.locf2}
\end{align}
where the second equality follows because $\sum^T_{j=1} I(D = j+1) =
1$, and the last equality (\ref{eq:two.locf2}) follows by a
conditioning argument (try it).

    
  \end{frame}

\begin{frame}
    \frametitle{LOCF in a longitudinal study}

\redbf{Example, continued:}  \blueem{Large sample behavior} of $\hatmu^{LOCF}_T$?
\begin{itemize}
\item Second equality follows because $\sum^T_{j=1} I(D = j+1) = 1$

\item Last equality (\ref{eq:two.locf2}) follows by a \blueem{conditioning
  argument} (try it)

\item \blueem{Define:}
$$\lambda_j(Z) = \pr(D=j|D \geq j, Z), \hspace*{0.05in} j=1,\ldots,T$$
$$\lambda_{T+1}(Z) = \pr(D = T+1 | D \geq T+1,Z) = 1$$
$$\overline{\pi}_j(Z) = \prod^j_{k=1} \{ 1 - \lambda_k(Z)\}, \hspace*{0.05in} j=1,\ldots,T$$
\end{itemize}
\vspace*{0.1in}

\redbf{Reexpress (\ref{eq:two.locf2}):}  Because (verify)
$$\pr(D = j+1|Z) =  \overline{\pi}_j(Z) \, \lambda_{j+1}(Z)$$
(\ref{eq:two.locf2}) becomes
\begin{equation}
\hatmu^{LOCF}_T \inp  E(Y_T) - \sum^{T-1}_{j=1} E\{  \overline{\pi}_j(Z) \lambda_{j+1}(Z) 
(Y_T-Y_j) \} 
\label{eq:two.locf3}
\end{equation}
  \end{frame}

  \begin{frame}
    \frametitle{LOCF in a longitudinal study}

    \redbf{Example, continued:} What does (\ref{eq:two.locf3}) imply? 
    \begin{itemize}
    \item \blueem{Clearly:} $\hatmu^{LOCF}_T$ is \blueem{not
        consistent for} $\mu_T$ in general
\item \blueem{With no dropout:} $\pr(D = j+1|Z) = \overline{\pi}_j(Z) \lambda_{j+1}(Z) = 0$,
  $j=1,\ldots,T-1$, and $\hatmu^{LOCF}_T$ \blueem{is consistent}

  \item \blueem{With MCAR dropout:}  $\pr(D=j+1|Z)$
\blueem{does not depend on} $Z$ and $\lambda_j(Z)$ 
$\overline{\pi}_j(Z)$ are \blueem{constants}
$\lambda_j$ and $\overline{\pi}_j$ $ \Longrightarrow$   
(\ref{eq:two.locf3}) becomes
\begin{equation}
\hatmu^{LOCF}_T \inp  E(Y_T) - \sum^{T-1}_{j=1} \overline{\pi}_j \lambda_{j+1} E(Y_T-Y_j)
\label{eq:two.locf4}
\end{equation}


      \end{itemize}

    
  \end{frame}

  \begin{frame}
    \frametitle{LOCF in a longitudinal study}

\redbf{Example, continued:}  (\ref{eq:two.locf4}) implies 
\begin{itemize}
\item If $E(Y_j)$ is \blueem{the same} for all $j$, then the second term in 
  (\ref{eq:two.locf4}) is \blueem{equal to zero} $
  \Longrightarrow$ $\hatmu^{LOCF}_T$ is \blueem{consistent}
\item If outcomes \blueem{increase over time}, $E(Y_{j+1}) \geq
  E(Y_j)$, $j=1,\ldots,T-1$, the second term in
  (\ref{eq:two.locf4}) is \blueem{nonnegative} $
  \Longrightarrow$ $\hatmu^{LOCF}_T \inp$ to a value
  \blueem{$\leq \mu_T$} and is \blueem{inconsistent}

\item \blueem{Thus:} In this special case, under MCAR, if 
$\mu_T$ is the expected outcome under \blueem{experimental
  treatment}, $\hatmu^{LOCF}_T $ is ``\blueem{conservative}'' 
\item (\ref{eq:two.locf4}) shows that \blueem{even under
    MCAR}, $\hatmu^{LOCF}_T$ is \blueem{not consistent} in general

  \item \blueem{Clearly:}  From  (\ref{eq:two.locf4}), under MAR or
    MNAR, $\hatmu^{LOCF}_T$ is \blueem{not consistent} in general
\end{itemize}

\vspace*{0.1in}

\redbf{Moral:} LOCF is an \blueem{ad hoc method} that, despite its
  simplicity and supposed interpretations, is not based on a
  principled framework, so is not discussed further
\end{frame}

 \subsubsection{Discussion}

\begin{frame}
\frametitle{2.  Na\"{i}ve Methods}

\redbf{2.5.  Discussion}
\end{frame}
  
  \begin{frame}
    \frametitle{Final thoughts}

    \redbf{Takeaway message:} Ad hoc approaches to accounting for
    missing data that are \blueem{not based on a formal, principled
      statistical framework} are likely to lead to \blueem{erroneous
      inferences}
    \begin{itemize}
\item  These approaches are best behaved under MCAR, but MCAR is
  \blueem{unlikely in practice}

\item \blueem{Moving forward:} In the next three chapters, we cover
  in detail \blueem{three main, principled approaches} to inference in the
  presence of missing data \blueem{under a MAR mechanism}
      \end{itemize}
    
    \end{frame}


    \subsubsection{Example of a table }

    \begin{frame}
  \frametitle{Modeling strategies for selection models}
\phantomsection\label{slide:twobytwo}

  
\redbf{Example 1 - bivariate binary outcome:}  \blueem{Observed data}
on $N$ individuals can be summarized as
\begin{itemize}
\item  For $R=0$ ($Y_2$ \blueem{missing}\,)
  $$m_0 = \sumiN I(Y_{1i}=0, R_{1i}=0), \hspace{0.1in} 
m_1 = \sumiN I(Y_{1i}=1, R_{1i}=0), \hspace{0.1in} m = m_0+m_1$$

\item For $R=1$ ($Y_2$ \blueem{observed}\,)

   \begin{center}
     \begin{tabular}{| c | c| c| c |}
       \hline
       $Y_1$ / $Y_2$    &   $Y_2=0$ & $Y_2=1$ &  Total \\\hline
   $Y_1=0$ &  $n_{00}$ &$ n_{01}$ & $n_{0 \cdot}$  \\ \hline
   $Y_1=1$ &   $n_{10}$ &$ n_{11}$ & $n_{1 \cdot}$ \\ \hline
  Total & $n_{\cdot 0}$ & $n_{\cdot 1}$ & $n$ \\ \hline  
     \end{tabular}
   \end{center}
%   \vspace*{0.15in}
  
   \item $N = n + m = n_{00} + n_{01} + n_{10} + n_{11} + m_0 + m_1$
   \end{itemize}


 \end{document}